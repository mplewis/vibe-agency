#!/usr/bin/env python3
"""
Manual E2E Test: PLANNING Workflow with File-Based Delegation

This script runs the PLANNING workflow and pauses for manual responses.

How to use:
1. Run this script: python3 manual_planning_test.py
2. When it pauses, read the request file shown
3. Write a response file manually (or use the helper below)
4. The workflow continues automatically

This tests the COMPLETE integration:
- CoreOrchestrator ‚úì
- PlanningHandler ‚úì
- PromptRegistry ‚úì
- File-based delegation ‚úì
- All PLANNING agents ‚úì
"""

import json
import sys
from datetime import datetime
from pathlib import Path

# Add orchestrator to path
sys.path.insert(0, str(Path(__file__).parent / "agency_os/00_system/orchestrator"))
sys.path.insert(0, str(Path(__file__).parent / "agency_os/00_system/runtime"))

from core_orchestrator import CoreOrchestrator, ProjectManifest, ProjectPhase, PlanningSubState


def show_request_file(request_file: Path):
    """Display request file for manual inspection"""
    print("\n" + "="*70)
    print("üì¨ INTELLIGENCE REQUEST")
    print("="*70)

    with open(request_file) as f:
        request = json.load(f)

    print(f"\nü§ñ Agent: {request['agent']}")
    print(f"üìã Task: {request['task_id']}")
    print(f"üïê Timestamp: {request['timestamp']}")
    print(f"\nüìÑ Request file: {request_file}")

    # Show first 500 chars of prompt
    prompt = request['prompt']
    print(f"\nüìù Prompt preview (first 500 chars):")
    print("-"*70)
    print(prompt[:500])
    if len(prompt) > 500:
        print(f"\n... ({len(prompt) - 500} more chars)")
    print("-"*70)

    return request


def create_mock_response(request_file: Path, request: dict):
    """Helper to create mock response file"""

    # Determine response based on agent
    agent = request['agent']
    task_id = request['task_id']

    print(f"\nü§î Generating mock response for {agent}.{task_id}...")

    # Mock responses for different agents
    if agent == "VIBE_ALIGNER":
        if "education" in task_id:
            result = {
                "education_complete": True,
                "user_understanding": "User understands the process"
            }
        elif "feature" in task_id:
            result = {
                "features": [
                    "User authentication",
                    "Dashboard",
                    "Data export"
                ],
                "extracted_requirements": "Basic MVP features identified"
            }
        elif "feasibility" in task_id:
            result = {
                "technical_feasibility": "HIGH",
                "business_feasibility": "MEDIUM",
                "recommendations": "Proceed with MVP"
            }
        elif "gap" in task_id:
            result = {
                "gaps": [],
                "clarifications_needed": []
            }
        elif "scope" in task_id:
            result = {
                "mvp_features": ["User auth", "Dashboard"],
                "deferred_features": ["Data export"],
                "negotiation_complete": True
            }
        else:
            result = {"status": "success", "data": f"Mock response for {task_id}"}

    elif agent == "MARKET_RESEARCHER":
        result = {
            "market_size": "Large addressable market",
            "competitors": ["Competitor A", "Competitor B"],
            "market_trends": "Growing demand for solution"
        }

    elif agent == "TECH_RESEARCHER":
        result = {
            "tech_stack_recommendation": "Python + React",
            "architecture": "Serverless",
            "considerations": "Scalability is key"
        }

    elif agent == "FACT_VALIDATOR":
        result = {
            "validation_status": "VERIFIED",
            "issues": [],
            "confidence": "HIGH"
        }

    elif agent == "USER_RESEARCHER":
        result = {
            "user_insights": "Users need simple interface",
            "pain_points": ["Complex workflows", "Slow performance"],
            "preferences": "Mobile-first design"
        }

    elif agent == "LEAN_CANVAS_VALIDATOR":
        result = {
            "canvas_valid": True,
            "problem": "Users struggle with X",
            "solution": "Our product solves X",
            "unique_value": "Simplicity + Speed",
            "validation_score": 8
        }

    elif agent == "GENESIS_BLUEPRINT":
        result = {
            "feature_spec": {
                "features": [
                    {"name": "Authentication", "priority": "P0"},
                    {"name": "Dashboard", "priority": "P0"}
                ],
                "user_stories": ["As user, I can login", "As user, I can view dashboard"]
            }
        }

    else:
        result = {
            "status": "success",
            "message": f"Mock response for {agent}",
            "data": "Generated by mock responder"
        }

    # Write response file
    response_file = request_file.parent / f"response_{request['request_id']}.json"
    response = {"result": result}

    with open(response_file, 'w') as f:
        json.dump(response, f, indent=2)

    print(f"‚úÖ Response written: {response_file}")
    print(f"üìä Result: {json.dumps(result, indent=2)[:200]}...")

    return response_file


def main():
    print("\n" + "="*70)
    print("üß™ MANUAL E2E TEST: PLANNING Workflow")
    print("="*70)

    # Create test project
    project_id = "manual-test-project"
    user_input = """
    Build a simple project management tool for small teams.

    Key features:
    - User authentication
    - Project dashboard
    - Task tracking
    - Team collaboration
    - Data export

    Target: Small teams (5-10 people)
    Timeline: 3 months MVP
    Budget: Limited
    """

    print(f"\nüìã Project ID: {project_id}")
    print(f"üìù User input: {user_input[:100]}...")

    # Initialize orchestrator
    print(f"\nüöÄ Initializing CoreOrchestrator (delegated mode)...")
    orchestrator = CoreOrchestrator(
        repo_root=Path.cwd(),
        execution_mode="delegated"
    )

    print("‚úÖ Orchestrator initialized")

    # Create workspace directory
    workspace_dir = Path("workspaces") / project_id
    workspace_dir.mkdir(parents=True, exist_ok=True)
    print(f"‚úÖ Workspace created: {workspace_dir}")

    # Create manifest JSON (initial)
    print(f"\nüì¶ Creating project manifest...")
    manifest_data = {
        "apiVersion": "agency.os/v1alpha1",
        "kind": "Project",
        "metadata": {
            "projectId": project_id,
            "name": project_id,
            "owner": "manual-test",
            "createdAt": datetime.now().isoformat() + "Z",
            "sdlc_version": "1.0",
            "orchestrator_version": "1.0",
        },
        "status": {
            "projectPhase": "PLANNING",
            "planningSubState": None,
            "lastUpdated": datetime.now().isoformat() + "Z",
        },
        "artifacts": {"user_input": user_input},
        "budget": {
            "max_cost_usd": 10.0,
            "current_cost_usd": 0.0,
            "alert_threshold": 0.80,
            "cost_breakdown": {},
        },
    }

    # Write manifest to disk
    manifest_path = workspace_dir / "project_manifest.json"
    with open(manifest_path, "w") as f:
        json.dump(manifest_data, f, indent=2)
    print(f"‚úÖ Manifest written: {manifest_path}")

    # Load manifest (orchestrator will validate it)
    manifest = orchestrator.load_project_manifest(project_id)
    print(f"‚úÖ Manifest loaded: phase={manifest.current_phase.value}")

    # Start planning workflow
    print(f"\nüéØ Starting PLANNING workflow...")
    print("   This will execute all PLANNING sub-states")
    print("   Auto-responder will handle delegation requests")
    print()

    try:
        # Execute PLANNING phase (this triggers all sub-states)
        print("üì§ Calling: orchestrator.execute_phase(manifest)")
        orchestrator.execute_phase(manifest)

        print("\n" + "="*70)
        print("‚úÖ PLANNING WORKFLOW COMPLETE!")
        print("="*70)

        print(f"\nüìä Final manifest:")
        print(f"   Phase: {manifest.current_phase}")
        print(f"   Sub-state: {manifest.current_sub_state}")
        print(f"   Artifacts: {len(manifest.artifacts)} items")

        # Show artifacts
        print(f"\nüì¶ Artifacts created:")
        for key, value in manifest.artifacts.items():
            if isinstance(value, dict):
                print(f"   - {key}: {len(value)} items")
            else:
                print(f"   - {key}: {type(value).__name__}")

        return True

    except Exception as e:
        print(f"\n‚ùå Workflow failed: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    # Check if we should use mock auto-responder
    import threading
    import time

    def auto_responder():
        """Automatically respond to delegation requests"""
        workspace_dir = Path("workspaces/manual-test-project/.delegation")

        print("\n[AUTO-RESPONDER] ü§ñ Starting auto-responder thread...")

        while True:
            if not workspace_dir.exists():
                time.sleep(0.1)
                continue

            # Look for request files
            request_files = list(workspace_dir.glob("request_*.json"))

            for request_file in request_files:
                # Check if response already exists
                request_id = request_file.stem.replace("request_", "")
                response_file = workspace_dir / f"response_{request_id}.json"

                if response_file.exists():
                    continue

                print(f"\n[AUTO-RESPONDER] üì¨ Found request: {request_file.name}")

                # Read request
                with open(request_file) as f:
                    request = json.load(f)

                # Show request
                show_request_file(request_file)

                # Create mock response
                create_mock_response(request_file, request)

                print(f"[AUTO-RESPONDER] ‚úÖ Response ready, workflow continues...")

            time.sleep(0.2)

    # Start auto-responder in background
    responder_thread = threading.Thread(target=auto_responder, daemon=True)
    responder_thread.start()

    # Give responder time to start
    time.sleep(0.5)

    # Run main workflow
    success = main()

    sys.exit(0 if success else 1)
