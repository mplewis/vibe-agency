# ğŸ¤– FOR AI AGENTS & ASSISTANTS - START HERE

**If you're an AI agent (Claude Code, GitHub Copilot, etc.) working on this codebase, read this FIRST.**

---

## âš ï¸ CRITICAL: You Are The Operator

**THIS IS NOT AN AUTONOMOUS SYSTEM.**

### Wrong Mental Model âŒ
```
Human â†’ ./vibe-cli run project â†’ vibe-cli does work autonomously â†’ outputs results
                                   â†“
                            "delegates to Claude Code"
```

### Correct Mental Model âœ…
```
Claude Code (YOU) â†’ ./vibe-cli run project â†’ vibe-cli writes prompt to .delegation/request_*.json
                                              â†“
                      YOU read request file â†’ YOU execute task â†’ YOU write result to .delegation/response_*.json
                                                                 â†“
                                                       vibe-cli reads & saves artifacts
```

**You are not a "service" that vibe-cli calls. You are the OPERATOR using vibe-cli as a tool.**

---

## ğŸ¯ What This System Actually Is

**Vibe Agency = File-based prompt composition framework**

- **NOT:** Multi-agent AI system, autonomous workflow engine, code generator
- **IS:** Structured prompt library with 6,400+ lines of domain knowledge

### Architecture
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  YOU (Claude Code / AI Operator)         â”‚
â”‚  â€¢ Execute ALL intelligent tasks         â”‚
â”‚  â€¢ Read prompts from vibe-cli            â”‚
â”‚  â€¢ Make decisions, do research, write    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ calls        â–² returns prompts
           â–¼              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  vibe-cli â†’ core_orchestrator.py         â”‚
â”‚  â€¢ State machine (PLANNINGâ†’CODINGâ†’etc)   â”‚
â”‚  â€¢ Prompt composition from templates     â”‚
â”‚  â€¢ Artifact storage                      â”‚
â”‚  â€¢ NO LLM calls, NO autonomous behavior  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Agents (File-based Prompt Templates)    â”‚
â”‚  â€¢ VIBE_ALIGNER/                         â”‚
â”‚  â€¢ GENESIS_BLUEPRINT/                    â”‚
â”‚  â€¢ MARKET_RESEARCHER/                    â”‚
â”‚  â€¢ etc.                                  â”‚
â”‚  â€¢ Just YAML/MD files with prompts       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‹ Before You Start Any Task

### 1. Read Operational Truth
**â†’ [CLAUDE.md](./CLAUDE.md)** - Current system status, what works, what doesn't

Contains:
- âœ… Components with **passing tests** (trust these)
- âš ï¸ Components with **code but no tests** (be careful)
- âŒ Known issues and gaps
- ğŸ” Verification commands to check claims

**Example verification:**
```bash
# Don't trust "PLANNING works" until you run:
python tests/test_planning_workflow.py

# Don't trust "vibe-cli has tool loop" until you check:
grep -n "tool_use" vibe-cli
```

### 2. Read Architecture Intent
**â†’ [ARCHITECTURE_V2.md](./ARCHITECTURE_V2.md)** - How system SHOULD work (conceptual)

**Warning:** This describes the INTENDED design. Reality may differ. When in doubt, trust tests > code > docs.

### 3. Check What Changed Recently
```bash
git log --oneline -10  # Recent commits
cat CHANGELOG.md       # Documented changes
```

---

## ğŸš« Common Anti-Patterns (DON'T DO THESE)

### âŒ Anti-Pattern 1: Assuming vibe-cli Is Autonomous
```python
# WRONG - Test that expects vibe-cli to "run" autonomously
def test_yoga_mvp():
    result = subprocess.run(["./vibe-cli", "run", "yoga-studio-mvp-001"])
    assert result.returncode == 0  # âŒ Will hang forever waiting for YOU
```

```python
# CORRECT - Test that simulates operator interaction
def test_yoga_mvp():
    # Orchestrator composes prompt
    prompt = orchestrator.get_next_prompt()

    # YOU (or test mock) execute prompt
    result = your_llm_call(prompt)

    # Feed result back
    orchestrator.process_result(result)
```

### âŒ Anti-Pattern 2: Creating Docs That Say "Ensure Claude Code Is Running"
```markdown
## Prerequisites
- Ensure Claude Code operator is running and ready to respond
```

**This makes no sense.** If you're reading this, YOU are Claude Code. You're already running.

**Correct version:**
```markdown
## How To Use This System As An AI Operator

1. You execute: `./vibe-cli run project-id`
2. You receive: A prompt file at `.delegation/request_*.json`
3. You do: The task described in the prompt
4. You write: Results to `.delegation/response_*.json`
```

### âŒ Anti-Pattern 3: Trusting Docs Without Verification
```python
# README says "CODING phase works"
# WRONG: Assume it works and write tests based on that

# CORRECT: Verify first
!python tests/test_coding_workflow.py  # Does this pass?
!grep -r "CODING" agency_os/  # Does code actually exist?
```

### âŒ Anti-Pattern 4: Confusing "Code Exists" With "Works"
```
# File exists: agency_os/core_system/orchestrator/handlers/testing_handler.py
# Lines: 108
# Status: âš ï¸ STUB ONLY (allows transitions, no real functionality)

# WRONG: "Testing handler is implemented"
# CORRECT: "Testing handler stub exists, needs implementation"
```

---

## âœ… Correct Workflows

### Scenario 1: User Asks "Run the yoga studio test"

**Wrong approach:**
```bash
./vibe-cli run yoga-studio-mvp-001
# âŒ Hangs waiting for you to respond to prompts
```

**Correct approach:**
```bash
# 1. Get initial prompt
./vibe-cli run yoga-studio-mvp-001
# â†’ Outputs: "You are VIBE_ALIGNER. Validate this business idea..."

# 2. YOU execute the task (using your LLM capabilities)
# - Read the prompt
# - Do research if needed
# - Make decisions
# - Compose response

# 3. Provide results back
# - Write to .delegation/response_*.json in the workspace directory
# - vibe-cli polls, reads the file, and processes results

# Repeat until project complete
```

### Scenario 2: User Asks "Write tests for the CODING phase"

**Correct steps:**
```bash
# 1. Verify CODING phase implementation exists
ls -la agency_os/core_system/orchestrator/handlers/coding_handler.py

# 2. Check if tests already exist
ls -la tests/test_coding_workflow.py

# 3. Run existing tests to understand current state
python tests/test_coding_workflow.py

# 4. Read CLAUDE.md to see documented status
grep -A 10 "CODING" CLAUDE.md

# 5. Only THEN write/update tests based on REALITY
```

### Scenario 3: User Asks "Implement feature X"

**Correct steps:**
```bash
# 1. Search if X already exists
find . -name "*X*" -type f
grep -r "X" agency_os/

# 2. Check architecture docs for intended design
grep -r "X" ARCHITECTURE_V2.md

# 3. If exists but untested:
#    â†’ Write tests first
#    â†’ Then fix if broken

# 4. If doesn't exist:
#    â†’ Follow architecture patterns
#    â†’ Write tests alongside implementation
#    â†’ Update CLAUDE.md when tests pass
```

---

## ğŸ§ª Testing Philosophy

**"Don't trust âœ… Complete without passing tests"**

### Evidence Hierarchy (strongest to weakest):
1. **Passing tests** = Works NOW (highest trust)
2. **Code exists** = Implemented, unknown if works
3. **Docs say "complete"** = May be outdated (lowest trust)

**When docs contradict code:** Trust code
**When code contradicts tests:** Trust tests
**When tests fail:** Status is âŒ BROKEN, not âœ… Complete

### Example:
```yaml
# CLAUDE.md says:
CODING Handler: âœ… Works (tested E2E)
Evidence: 3 tests pass (test_coding_workflow.py)
Verify: python3 -m pytest tests/test_coding_workflow.py -v

# You verify:
$ python3 -m pytest tests/test_coding_workflow.py -v
# â†’ 3 passed âœ…

# Conclusion: Trust this claim âœ…
```

---

## ğŸ“ Key Files For AI Agents

**Read in this order:**

1. **[CLAUDE.md](./CLAUDE.md)** (5 min read)
   - What works NOW
   - What's broken
   - What's missing
   - Verification commands

2. **[ARCHITECTURE_V2.md](./ARCHITECTURE_V2.md)** (10 min read)
   - How system SHOULD work
   - Design decisions
   - Component relationships

3. **[README.md](./README.md)** (5 min read)
   - Human-facing overview
   - Setup instructions
   - Quick start (but remember: YOU are the operator!)

4. **tests/** directory
   - Source of truth for "what works"
   - Examples of correct usage
   - Edge cases and error handling

---

## ğŸ”„ Development Workflow For AI Agents

```bash
# 1. Understand task
git status  # Where are we?
git log --oneline -5  # What changed recently?

# 2. Verify current state
cat CLAUDE.md  # What's the operational truth?
make test  # What passes?

# 3. Make changes
# - Edit code
# - Write/update tests
# - Run tests until they pass

# 4. Update operational docs
# - Update CLAUDE.md status tables
# - Update "Last Verified" timestamp
# - Add verification commands

# 5. Commit with evidence
git commit -m "feat: Add X

Evidence: test_x.py passes
Verify: python tests/test_x.py"
```

---

## ğŸ†˜ When Things Don't Make Sense

### If documentation contradicts itself:
1. Run verification commands from CLAUDE.md
2. Trust: tests > code > CLAUDE.md > ARCHITECTURE_V2.md > README.md
3. Update docs to match reality

### If tests fail but docs say "complete":
1. Docs are outdated
2. Mark status as âŒ BROKEN in CLAUDE.md
3. Fix code OR fix test (whichever is wrong)
4. Update docs only when tests pass

### If you're not sure what a component does:
1. Read its tests first (`tests/test_X.py`)
2. Then read code (`agency_os/**/X.py`)
3. Then read docs (may be conceptual, not literal)

---

## ğŸ¯ Quick Reference

**I want to...**

- **Understand system status** â†’ [CLAUDE.md](./CLAUDE.md)
- **Understand architecture** â†’ [ARCHITECTURE_V2.md](./ARCHITECTURE_V2.md)
- **Run tests** â†’ `make test`
- **Check what works** â†’ `python tests/test_*.py`
- **Verify a claim** â†’ Run verification command from CLAUDE.md
- **Add a feature** â†’ Write test first, then implement
- **Fix a bug** â†’ Write failing test, fix code, test passes
- **Update docs** â†’ Only after tests pass

**Remember:**
- You are the OPERATOR
- vibe-cli returns PROMPTS, not results
- Tests are source of truth
- When in doubt, RUN THE VERIFICATION COMMAND

---

## ğŸ“ Getting Help

If you're still confused after reading this:

1. Check if issue is already in CLAUDE.md "Known Issues" section
2. Run meta-verification test: `bash tests/verify_claude_md.sh` (if exists)
3. Ask user for clarification on architecture intent
4. Search for similar patterns in existing tests

**DO NOT:**
- Assume system works like other AI agent frameworks
- Trust "autonomous" or "multi-agent" terminology without verification
- Write tests that expect vibe-cli to run without operator input
- Create docs that treat you as an external service

---

**Last Updated:** 2025-11-15
**Next Agent:** Please update this file if you find it inaccurate or incomplete
**Verification:** This doc makes claims about architecture - verify by reading vibe-cli source
