# GAD-004: Multi-Layered Quality Enforcement System

**Status:** ‚úÖ Approved
**Date:** 2025-11-16
**Authors:** Claude Code (System Architect)
**Supersedes:** Gemini's "Runtime Feedback Loop" proposal (rejected)
**Related:** GAD-002 (Core SDLC Orchestration), ADR-003 (Delegated Execution)

---

## Executive Summary

This document defines the **Multi-Layered Quality Enforcement System** - a comprehensive, defense-in-depth approach to ensuring code quality, workflow integrity, and production readiness across all phases of the SDLC.

**The Problem:** Current linting enforcement is manual (CLAUDE.md checklist), leading to CI/CD failures when agents forget to run checks. Quality gates exist for workflow-level concerns (security, compliance) but lack durable state tracking.

**The Solution:** A **3-layer enforcement system** that operates at different scopes:

1. **Layer 1: Session-Scoped Enforcement** (Development-Time)
   Prevents bad commits via pre-push validation and system status visibility.

2. **Layer 2: Workflow-Scoped Quality Gates** (Runtime)
   Records AUDITOR results in `project_manifest.json` for auditability and async remediation.

3. **Layer 3: Deployment-Scoped Validation** (Post-Merge)
   GitHub Actions E2E tests validate production readiness.

**Impact:** Eliminates manual quality checks, provides full auditability, enables async remediation, and prevents low-quality code from entering the codebase.

---

## Table of Contents

1. [Context & Problem Statement](#1-context--problem-statement)
2. [Decision: The 3-Layer System](#2-decision-the-3-layer-system)
3. [BLUEPRINT: Architectural Design](#3-blueprint-architectural-design)
4. [IMPLEMENTATION: Concrete Code](#4-implementation-concrete-code)
5. [HARNESS: Testing & Verification](#5-harness-testing--verification)
6. [Rollout Plan](#6-rollout-plan)
7. [Success Metrics](#7-success-metrics)
8. [Appendix: Rejected Alternatives](#8-appendix-rejected-alternatives)

---

## 1. Context & Problem Statement

### 1.1. Current State

**Session Handoff System (‚úÖ Implemented):**
- `.session_handoff.json` - Manual handoffs between agents
- `.system_status.json` - Auto-updated git/test status
- `bin/show-context.sh` - ONE COMMAND for full context

**Quality Gates (‚úÖ Implemented):**
- Workflow-level gates (security, compliance) block state transitions
- Defined in `ORCHESTRATION_workflow_design.yaml`
- Executed via AUDITOR agent in `core_orchestrator.py`

**Linting Enforcement (‚ùå Manual):**
- CLAUDE.md checklist: "Run `uv run ruff check . --fix` before commit"
- Agents can forget ‚Üí CI/CD failure (too late)
- No runtime enforcement, no automatic prevention

### 1.2. The Fundamental Problem

**Scope Confusion:** Different quality concerns operate at different scopes.

| Concern | Scope | Lifetime | Current Enforcement |
|---------|-------|----------|---------------------|
| Linting errors | **COMMIT** | Transient (fixed per commit) | ‚ùå Manual checklist |
| Unit tests | **SESSION** | Ephemeral (run per development) | ‚ö†Ô∏è System status (visibility only) |
| Security scans | **PROJECT** | Durable (affects workflow) | ‚úÖ Quality gates (blocking) |
| E2E tests | **DEPLOYMENT** | Permanent (production readiness) | ‚úÖ GitHub Actions |

**The Gap:** No automatic enforcement at COMMIT/SESSION scope.

### 1.3. Rejected Alternative: Gemini's Proposal

Gemini proposed storing linting results in `project_manifest.json`:

```json
"status": {
  "qualityGates": {
    "T1_StartTesting": {
      "status": "FAIL",
      "check": "ruff_linting_check",
      "message": "12 errors"
    }
  }
}
```

**Why Rejected:**

1. **Scope Confusion:** Linting errors are COMMIT-scoped (transient), not PROJECT-scoped (durable)
2. **Manifest Pollution:** Multiple fix attempts create bloated history
3. **Wrong Layer:** Orchestrator runs AFTER code is committed (too late)
4. **Architectural Mismatch:** Delegated execution = Orchestrator is remote, not local

**What We Accepted:** Runtime Feedback Loop for WORKFLOW-level gates (security, compliance)

---

## 2. Decision: The 3-Layer System

### 2.1. Layered Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ LAYER 1: Session-Scoped Enforcement                        ‚îÇ
‚îÇ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÇ
‚îÇ Scope:   COMMIT/SESSION (per development cycle)             ‚îÇ
‚îÇ Target:  Linting, formatting, unit tests                    ‚îÇ
‚îÇ Tools:   .system_status.json, bin/pre-push-check.sh         ‚îÇ
‚îÇ Effect:  PREVENTS bad commits (pre-push enforcement)        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚Üì (clean code only)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ LAYER 2: Workflow-Scoped Quality Gates                     ‚îÇ
‚îÇ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÇ
‚îÇ Scope:   PROJECT (per workflow execution)                   ‚îÇ
‚îÇ Target:  Security, compliance, architecture validation      ‚îÇ
‚îÇ Tools:   manifest.status.qualityGates{}, AUDITOR agent      ‚îÇ
‚îÇ Effect:  BLOCKS workflow transitions, enables async remediation ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚Üì (after PR merge)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ LAYER 3: Deployment-Scoped Validation                      ‚îÇ
‚îÇ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÇ
‚îÇ Scope:   DEPLOYMENT (per merge to main)                     ‚îÇ
‚îÇ Target:  E2E tests, integration tests, performance          ‚îÇ
‚îÇ Tools:   GitHub Actions, staging environment                ‚îÇ
‚îÇ Effect:  VALIDATES production readiness                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 2.2. Design Principles

1. **Right Scope, Right Tool:** Each layer uses appropriate tooling for its scope
2. **Defense in Depth:** Multiple layers provide redundancy
3. **Early Detection:** Issues caught at earliest possible layer
4. **Durable State:** Critical decisions persisted for auditability
5. **Async-Ready:** Layer 2 enables future async remediation agents

---

## 3. BLUEPRINT: Architectural Design

### 3.1. Layer 1: Session-Scoped Enforcement

#### 3.1.1. Components

**A. System Status Extension**

File: `.system_status.json` (auto-generated, gitignored)

```json
{
  "timestamp": "2025-11-16T15:30:00Z",
  "git": {
    "branch": "claude/feature-123",
    "last_commit": { "sha": "abc123", "message": "feat: ..." },
    "working_directory_clean": true
  },
  "tests": {
    "planning_workflow": "passing"
  },
  "linting": {
    "status": "failing",
    "errors_count": 12,
    "last_checked": "2025-11-16T15:29:45Z"
  },
  "formatting": {
    "status": "passing",
    "last_checked": "2025-11-16T15:29:50Z"
  },
  "session_handoff_exists": true
}
```

**B. Pre-Push Check Script**

File: `bin/pre-push-check.sh`

```bash
#!/usr/bin/env bash
# MANDATORY checks before git push
# Blocks push if critical checks fail

Checks:
1. Linting (ruff check)
2. Formatting (ruff format --check)
3. System status update

Exit Codes:
  0 = All checks passed (safe to push)
  1 = Check failed (push blocked)
```

**C. Display Integration**

File: `bin/show-context.sh` (extend existing)

```
Shows:
  Linting: ‚úÖ Passing | ‚ùå failing (12 errors)
  Formatting: ‚úÖ Passing
```

#### 3.1.2. Workflow

```
Agent writes code
  ‚Üì
Agent runs: ./bin/pre-push-check.sh
  ‚Üì
Check 1: Linting ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí [FAIL] ‚Üí Shows error count + fix command
  ‚Üì                                   EXIT 1 (push blocked)
Check 2: Formatting ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí [FAIL] ‚Üí Shows fix command
  ‚Üì                                   EXIT 1 (push blocked)
Check 3: Update status ‚îÄ‚îÄ‚Üí [SUCCESS]
  ‚Üì
ALL PASSED
  ‚Üì
git push (allowed)
```

#### 3.1.3. Integration Points

- **Manual:** `./bin/pre-push-check.sh && git push`
- **Git Hook:** `.githooks/pre-push` calls script (optional)
- **CLAUDE.md:** Updated to reference pre-push-check.sh
- **show-context.sh:** Displays linting/formatting status

---

### 3.2. Layer 2: Workflow-Scoped Quality Gates

#### 3.2.1. Manifest Extension

File: `project_manifest.json` (project-scoped, committed)

```json
{
  "apiVersion": "agency.os/v1alpha1",
  "kind": "Project",
  "metadata": { "projectId": "...", "name": "..." },
  "status": {
    "projectPhase": "CODING",
    "lastUpdate": "2025-11-16T15:00:00Z",
    "qualityGates": {
      "T1_StartCoding": {
        "transition": "PLANNING.ARCHITECTURE_DESIGN ‚Üí CODING",
        "gates": [
          {
            "check": "prompt_security_scan",
            "status": "PASS",
            "findings": 0,
            "timestamp": "2025-11-16T14:55:00Z",
            "duration_ms": 1234
          },
          {
            "check": "feature_spec_validation",
            "status": "PASS",
            "findings": 0,
            "timestamp": "2025-11-16T14:55:05Z",
            "duration_ms": 890
          }
        ]
      },
      "T4_StartDeployment": {
        "transition": "AWAITING_QA_APPROVAL ‚Üí DEPLOYMENT",
        "gates": [
          {
            "check": "code_security_scan",
            "status": "FAIL",
            "severity": "critical",
            "blocking": true,
            "findings": 3,
            "message": "Found 3 critical vulnerabilities: SQL injection in auth.py:42, XSS in render.py:89, hardcoded secret in config.py:12",
            "remediation": "Run AUDITOR agent with security_fix mode OR manually fix vulnerabilities",
            "timestamp": "2025-11-16T15:00:00Z",
            "duration_ms": 5678
          }
        ]
      }
    }
  },
  "artifacts": { ... }
}
```

#### 3.2.2. Orchestrator Changes

**File:** `agency_os/core_system/orchestrator/core_orchestrator.py`

**New Method:**

```python
def _record_quality_gate_result(
    self,
    manifest: ProjectManifest,
    transition_name: str,
    gate: Dict[str, Any],
    audit_report: Dict[str, Any]
) -> None:
    """
    Record quality gate result in manifest for auditability.

    This enables:
    - Durable state (persists after process ends)
    - Audit trail (all gate executions recorded)
    - Async remediation (external tools can read manifest and fix)

    Args:
        manifest: Project manifest to update
        transition_name: Name of transition (e.g., "T1_StartCoding")
        gate: Gate config from workflow YAML
        audit_report: Result from AUDITOR agent
    """
    # Initialize qualityGates structure if not exists
    if "qualityGates" not in manifest.metadata.get("status", {}):
        if "status" not in manifest.metadata:
            manifest.metadata["status"] = {}
        manifest.metadata["status"]["qualityGates"] = {}

    # Get or create transition record
    if transition_name not in manifest.metadata["status"]["qualityGates"]:
        # Extract transition info from workflow YAML
        transition = self._get_transition_config(transition_name)
        manifest.metadata["status"]["qualityGates"][transition_name] = {
            "transition": f"{transition['from_state']} ‚Üí {transition['to_state']}",
            "gates": []
        }

    # Build gate result record
    gate_result = {
        "check": gate["check"],
        "status": audit_report.get("status", "UNKNOWN"),
        "timestamp": datetime.utcnow().isoformat() + "Z",
        "duration_ms": audit_report.get("duration_ms", 0)
    }

    # Add optional fields
    if "severity" in gate:
        gate_result["severity"] = gate["severity"]
    if "blocking" in gate:
        gate_result["blocking"] = gate["blocking"]
    if "findings" in audit_report:
        gate_result["findings"] = audit_report["findings"]
    if "message" in audit_report:
        gate_result["message"] = audit_report["message"]
    if "remediation" in audit_report:
        gate_result["remediation"] = audit_report["remediation"]

    # Append to gates list
    manifest.metadata["status"]["qualityGates"][transition_name]["gates"].append(gate_result)

    # Save manifest immediately (durable state)
    self.save_project_manifest(manifest)

    logger.info(f"‚úì Recorded quality gate result: {gate['check']} = {gate_result['status']}")
```

**Modified Method:**

```python
def apply_quality_gates(self, transition_name: str, manifest: ProjectManifest) -> None:
    """
    Apply quality gates for a state transition.

    NEW: Records all gate results in manifest before blocking.
    """
    # ... (existing code to find transition) ...

    logger.info(f"üîí Applying quality gates for transition: {transition_name}")

    quality_gates = transition["quality_gates"]

    # Run ALL gates (blocking and non-blocking)
    # Record results BEFORE raising exceptions
    for gate in quality_gates:
        try:
            audit_report = self.invoke_auditor(
                check_type=gate["check"],
                manifest=manifest,
                severity=gate.get("severity", "critical"),
                blocking=False  # Don't raise exception yet
            )

            # RECORD RESULT (new functionality)
            self._record_quality_gate_result(
                manifest=manifest,
                transition_name=transition_name,
                gate=gate,
                audit_report=audit_report
            )

            # NOW check if we should block
            if gate.get("blocking", False) and audit_report["status"] == "FAIL":
                raise QualityGateFailure(
                    f"Quality gate '{gate['check']}' FAILED (severity={gate.get('severity')})\n"
                    f"Findings: {audit_report.get('findings', 'N/A')}\n"
                    f"Message: {audit_report.get('message', 'N/A')}\n"
                    f"Remediation: {audit_report.get('remediation', 'See audit report')}"
                )

        except QualityGateFailure:
            # Gate failed - result already recorded in manifest
            # Re-raise to block transition
            raise
        except Exception as e:
            # Unexpected error - record as ERROR status
            error_report = {
                "status": "ERROR",
                "message": str(e),
                "timestamp": datetime.utcnow().isoformat() + "Z"
            }
            self._record_quality_gate_result(
                manifest=manifest,
                transition_name=transition_name,
                gate=gate,
                audit_report=error_report
            )

            if gate.get("blocking", False):
                raise

    logger.info(f"‚úÖ Quality gates passed for: {transition_name}")
```

#### 3.2.3. AUDITOR Agent Enhancement

**File:** `agency_os/01_planning_framework/agents/AUDITOR/_composition.yaml`

Add remediation guidance to output schema:

```yaml
output_format:
  type: "json"
  schema:
    status: "PASS | FAIL | ERROR"
    findings: "number"
    message: "string (description of findings)"
    remediation: "string (actionable fix instructions)"  # NEW
    timestamp: "ISO8601"
    duration_ms: "number"
```

**Example AUDITOR output:**

```json
{
  "status": "FAIL",
  "findings": 3,
  "message": "Found 3 critical security vulnerabilities",
  "remediation": "1. Fix SQL injection in auth.py:42 - use parameterized queries\n2. Fix XSS in render.py:89 - sanitize user input\n3. Remove hardcoded secret in config.py:12 - use environment variables",
  "timestamp": "2025-11-16T15:00:00Z",
  "duration_ms": 5678
}
```

#### 3.2.4. Workflow Integration

**No changes needed to** `ORCHESTRATION_workflow_design.yaml`

Existing quality gates will now automatically record results:

```yaml
transitions:
  - name: "T1_StartCoding"
    from_state: "PLANNING.ARCHITECTURE_DESIGN"
    to_state: "CODING"
    quality_gates:  # These now record results in manifest
      - agent: "AUDITOR"
        check: "prompt_security_scan"
        severity: "critical"
        blocking: true
      - agent: "AUDITOR"
        check: "feature_spec_validation"
        severity: "high"
        blocking: true
```

---

### 3.3. Layer 3: Deployment-Scoped Validation

#### 3.3.1. GitHub Actions Workflow

**File:** `.github/workflows/post-merge-validation.yml` (new)

```yaml
name: Post-Merge Validation

on:
  push:
    branches:
      - main
      - develop

jobs:
  e2e-tests:
    name: E2E Tests on Staging
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install uv
          uv sync

      - name: Deploy to staging
        run: |
          # Deploy latest main to staging environment
          # (Implementation depends on deployment target)
          echo "Deploying to staging..."

      - name: Run E2E test suite
        run: |
          uv run pytest tests/e2e/ -v --tb=short

      - name: Run performance tests (non-blocking)
        continue-on-error: true
        run: |
          uv run pytest tests/performance/ -v

      - name: Report results
        if: always()
        uses: actions/github-script@v7
        with:
          script: |
            const status = '${{ job.status }}';
            const message = status === 'success'
              ? '‚úÖ Post-merge validation passed'
              : '‚ùå Post-merge validation failed';

            github.rest.repos.createCommitStatus({
              owner: context.repo.owner,
              repo: context.repo.repo,
              sha: context.sha,
              state: status === 'success' ? 'success' : 'failure',
              context: 'Post-Merge Validation',
              description: message
            });
```

#### 3.3.2. Optional: Environment Status File

**File:** `.environment_status.json` (optional, for advanced use)

```json
{
  "staging": {
    "last_deployment": {
      "sha": "abc123",
      "timestamp": "2025-11-16T15:00:00Z",
      "status": "success"
    },
    "last_validation": {
      "e2e_tests": "passing",
      "performance_tests": "warning",
      "timestamp": "2025-11-16T15:05:00Z"
    }
  },
  "production": {
    "last_deployment": {
      "sha": "def456",
      "timestamp": "2025-11-15T10:00:00Z",
      "status": "success"
    }
  }
}
```

---

## 4. IMPLEMENTATION: Concrete Code

### 4.1. Layer 1: Session-Scoped Enforcement

#### 4.1.1. Extend `bin/update-system-status.sh`

**Location:** Lines 28-36 (after test status check)

**Add:**

```bash
# Check linting status (ruff check)
LINTING_STATUS="unknown"
LINTING_ERRORS=0
if command -v uv &>/dev/null; then
  # Run ruff check and capture output
  LINTING_OUTPUT=$(uv run ruff check . --output-format=text 2>&1 || true)
  # Count errors (lines matching "*.py:")
  LINTING_ERRORS=$(echo "$LINTING_OUTPUT" | grep -c "^.*\.py:" || echo "0")

  if [ "$LINTING_ERRORS" -eq 0 ]; then
    LINTING_STATUS="passing"
  else
    LINTING_STATUS="failing"
  fi
else
  LINTING_STATUS="uv_not_available"
fi

# Check formatting status (ruff format)
FORMATTING_STATUS="unknown"
if command -v uv &>/dev/null; then
  if uv run ruff format --check . &>/dev/null; then
    FORMATTING_STATUS="passing"
  else
    FORMATTING_STATUS="failing"
  fi
else
  FORMATTING_STATUS="uv_not_available"
fi
```

**Extend JSON output (lines 45-63):**

```bash
cat > "$STATUS_FILE" <<EOF
{
  "timestamp": "$TIMESTAMP",
  "git": {
    "branch": "$CURRENT_BRANCH",
    "last_commit": {
      "sha": "$LAST_COMMIT",
      "message": "$LAST_COMMIT_MSG",
      "date": "$LAST_COMMIT_DATE"
    },
    "working_directory_clean": $WORKING_DIR_CLEAN
  },
  "tests": {
    "planning_workflow": "$TESTS_STATUS"
  },
  "linting": {
    "status": "$LINTING_STATUS",
    "errors_count": $LINTING_ERRORS
  },
  "formatting": {
    "status": "$FORMATTING_STATUS"
  },
  "session_handoff_exists": $SESSION_HANDOFF_EXISTS,
  "generated_by": "bin/update-system-status.sh"
}
EOF
```

#### 4.1.2. Extend `bin/show-context.sh`

**Location:** After line 67 (after tests display)

**Add:**

```bash
# Extract linting status
LINTING_STATUS=$(grep '"linting"' "$SYSTEM_STATUS" -A 2 | grep '"status"' | sed 's/.*: "\(.*\)".*/\1/' | head -1)
LINTING_ERRORS=$(grep '"errors_count"' "$SYSTEM_STATUS" | sed 's/.*: \([0-9]*\).*/\1/')

# Extract formatting status
FORMATTING_STATUS=$(grep '"formatting"' "$SYSTEM_STATUS" -A 1 | grep '"status"' | sed 's/.*: "\(.*\)".*/\1/' | head -1)

# Display linting status
if [ "$LINTING_STATUS" = "passing" ]; then
  echo "Linting: ‚úÖ Passing"
elif [ "$LINTING_STATUS" = "failing" ]; then
  echo "Linting: ‚ùå Failing ($LINTING_ERRORS errors)"
else
  echo "Linting: ‚ö†Ô∏è  $LINTING_STATUS"
fi

# Display formatting status
if [ "$FORMATTING_STATUS" = "passing" ]; then
  echo "Formatting: ‚úÖ Passing"
elif [ "$FORMATTING_STATUS" = "failing" ]; then
  echo "Formatting: ‚ùå Failing"
else
  echo "Formatting: ‚ö†Ô∏è  $FORMATTING_STATUS"
fi
```

#### 4.1.3. Create `bin/pre-push-check.sh`

**File:** `bin/pre-push-check.sh` (new file, executable)

```bash
#!/usr/bin/env bash
#
# pre-push-check.sh
# MANDATORY quality checks before git push
# Blocks push if critical checks fail
#
# Usage: ./bin/pre-push-check.sh
# Integration: ./bin/pre-push-check.sh && git push
# Git Hook: .githooks/pre-push (optional)
#
# Exit Codes:
#   0 = All checks passed (safe to push)
#   1 = Check failed (push blocked)

set -euo pipefail

echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"
echo "üîç PRE-PUSH QUALITY CHECKS"
echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"
echo ""

FAILED=0

# ============================================================================
# CHECK 1: Linting (ruff check)
# ============================================================================
echo "1Ô∏è‚É£  Checking linting (ruff check)..."

if ! command -v uv &>/dev/null; then
  echo "   ‚ö†Ô∏è  uv not available - skipping linting check"
else
  # Run ruff check and capture output
  if ! uv run ruff check . --output-format=github 2>&1 | tee /tmp/ruff-check.log; then
    ERRORS=$(grep -c "\.py:" /tmp/ruff-check.log 2>/dev/null || echo "0")
    echo ""
    echo "   ‚ùå LINTING FAILED: $ERRORS error(s) found"
    echo ""
    echo "   How to fix:"
    echo "     uv run ruff check . --fix      # Auto-fix most issues"
    echo "     uv run ruff check .            # Review remaining issues"
    echo ""
    FAILED=1
  else
    echo "   ‚úÖ Linting passed (0 errors)"
  fi
fi

echo ""

# ============================================================================
# CHECK 2: Formatting (ruff format)
# ============================================================================
echo "2Ô∏è‚É£  Checking formatting (ruff format --check)..."

if ! command -v uv &>/dev/null; then
  echo "   ‚ö†Ô∏è  uv not available - skipping formatting check"
else
  if ! uv run ruff format --check . &>/dev/null; then
    echo "   ‚ùå FORMATTING FAILED"
    echo ""
    echo "   How to fix:"
    echo "     uv run ruff format .           # Auto-format all files"
    echo ""
    FAILED=1
  else
    echo "   ‚úÖ Formatting passed"
  fi
fi

echo ""

# ============================================================================
# CHECK 3: Update system status
# ============================================================================
echo "3Ô∏è‚É£  Updating system status..."

if [ -f "bin/update-system-status.sh" ]; then
  if ./bin/update-system-status.sh &>/dev/null; then
    echo "   ‚úÖ System status updated"
  else
    echo "   ‚ö†Ô∏è  System status update failed (non-critical)"
  fi
else
  echo "   ‚ö†Ô∏è  bin/update-system-status.sh not found (skipping)"
fi

echo ""
echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"

# ============================================================================
# FINAL RESULT
# ============================================================================
if [ $FAILED -eq 1 ]; then
  echo "‚ùå PRE-PUSH CHECKS FAILED"
  echo ""
  echo "   Push blocked. Fix the errors above and try again."
  echo ""
  exit 1
else
  echo "‚úÖ ALL PRE-PUSH CHECKS PASSED"
  echo ""
  echo "   Safe to push!"
  echo ""
  exit 0
fi
```

**Make executable:**

```bash
chmod +x bin/pre-push-check.sh
```

#### 4.1.4. Update CLAUDE.md

**File:** `CLAUDE.md`

**Replace Section:** "üö® BEFORE EVERY COMMIT (MANDATORY)"

**New Content:**

```markdown
### **üö® BEFORE EVERY PUSH (MANDATORY - AUTOMATED)**

**CI/CD will FAIL if you skip this!**

```bash
# ONE COMMAND - runs all checks automatically
./bin/pre-push-check.sh && git push
```

**What this does:**
1. ‚úÖ Checks linting (ruff check)
2. ‚úÖ Checks formatting (ruff format --check)
3. ‚úÖ Updates system status
4. ‚úÖ BLOCKS push if any check fails

**If checks fail:**
```bash
# Fix linting errors
uv run ruff check . --fix

# Fix formatting issues
uv run ruff format .

# Re-run checks
./bin/pre-push-check.sh
```

**Why this is MANDATORY:**
- CI/CD runs `.github/workflows/validate.yml` on every push
- It runs `uv run ruff check . --output-format=github` (line 66)
- If ruff check fails ‚Üí CI/CD fails ‚Üí PR cannot merge
- **./bin/pre-push-check.sh prevents CI/CD failures** by catching issues BEFORE push
```

---

### 4.2. Layer 2: Workflow-Scoped Quality Gates

#### 4.2.1. Extend `core_orchestrator.py`

**File:** `agency_os/core_system/orchestrator/core_orchestrator.py`

**Add new method after line 1169 (after `run_horizontal_audits`):**

```python
def _get_transition_config(self, transition_name: str) -> Dict[str, Any]:
    """
    Get transition configuration from workflow YAML.

    Args:
        transition_name: Name of transition (e.g., "T1_StartCoding")

    Returns:
        Transition config dict with from_state, to_state, quality_gates

    Raises:
        ValueError: If transition not found
    """
    for transition in self.workflow.get("transitions", []):
        if transition["name"] == transition_name:
            return transition

    raise ValueError(f"Transition not found in workflow: {transition_name}")

def _record_quality_gate_result(
    self,
    manifest: ProjectManifest,
    transition_name: str,
    gate: Dict[str, Any],
    audit_report: Dict[str, Any]
) -> None:
    """
    Record quality gate result in manifest for auditability.

    This enables:
    - Durable state (persists after process ends)
    - Audit trail (all gate executions recorded)
    - Async remediation (external tools can read manifest and fix)

    Args:
        manifest: Project manifest to update
        transition_name: Name of transition (e.g., "T1_StartCoding")
        gate: Gate config from workflow YAML
        audit_report: Result from AUDITOR agent
    """
    # Initialize qualityGates structure if not exists
    if "qualityGates" not in manifest.metadata.get("status", {}):
        if "status" not in manifest.metadata:
            manifest.metadata["status"] = {}
        manifest.metadata["status"]["qualityGates"] = {}

    # Get or create transition record
    if transition_name not in manifest.metadata["status"]["qualityGates"]:
        # Extract transition info from workflow YAML
        try:
            transition = self._get_transition_config(transition_name)
            manifest.metadata["status"]["qualityGates"][transition_name] = {
                "transition": f"{transition['from_state']} ‚Üí {transition['to_state']}",
                "gates": []
            }
        except ValueError:
            # Fallback if transition not found
            manifest.metadata["status"]["qualityGates"][transition_name] = {
                "transition": transition_name,
                "gates": []
            }

    # Build gate result record
    gate_result = {
        "check": gate["check"],
        "status": audit_report.get("status", "UNKNOWN"),
        "timestamp": datetime.utcnow().isoformat() + "Z",
    }

    # Add optional fields
    if "duration_ms" in audit_report:
        gate_result["duration_ms"] = audit_report["duration_ms"]
    if "severity" in gate:
        gate_result["severity"] = gate["severity"]
    if "blocking" in gate:
        gate_result["blocking"] = gate["blocking"]
    if "findings" in audit_report:
        gate_result["findings"] = audit_report["findings"]
    if "message" in audit_report:
        gate_result["message"] = audit_report["message"]
    if "remediation" in audit_report:
        gate_result["remediation"] = audit_report["remediation"]

    # Append to gates list
    manifest.metadata["status"]["qualityGates"][transition_name]["gates"].append(gate_result)

    # Save manifest immediately (durable state)
    self.save_project_manifest(manifest)

    logger.info(f"‚úì Recorded quality gate result: {gate['check']} = {gate_result['status']}")
```

**Modify `apply_quality_gates` method (starting at line 1044):**

```python
def apply_quality_gates(self, transition_name: str, manifest: ProjectManifest) -> None:
    """
    Apply quality gates for a state transition.

    Implements GAD-002 Decision 2: Hybrid Blocking/Async Quality Gates
    NEW (GAD-004): Records all gate results in manifest before blocking

    Args:
        transition_name: Name of transition (e.g., "T1_StartCoding")
        manifest: Project manifest

    Raises:
        QualityGateFailure: If blocking quality gate fails
    """
    # Find transition in workflow
    transition = None
    for t in self.workflow.get("transitions", []):
        if t["name"] == transition_name:
            transition = t
            break

    if not transition or "quality_gates" not in transition:
        # No quality gates for this transition
        return

    logger.info(f"üîí Applying quality gates for transition: {transition_name}")

    quality_gates = transition["quality_gates"]

    # Run ALL gates (blocking and non-blocking)
    # Record results BEFORE raising exceptions (GAD-004 enhancement)
    for gate in quality_gates:
        try:
            # Execute AUDITOR agent
            audit_report = self.invoke_auditor(
                check_type=gate["check"],
                manifest=manifest,
                severity=gate.get("severity", "critical"),
                blocking=False  # Don't raise exception yet
            )

            # RECORD RESULT in manifest (GAD-004: new functionality)
            self._record_quality_gate_result(
                manifest=manifest,
                transition_name=transition_name,
                gate=gate,
                audit_report=audit_report
            )

            # NOW check if we should block
            if gate.get("blocking", False) and audit_report.get("status") == "FAIL":
                raise QualityGateFailure(
                    f"Quality gate '{gate['check']}' FAILED (severity={gate.get('severity')})\n"
                    f"Findings: {audit_report.get('findings', 'N/A')}\n"
                    f"Message: {audit_report.get('message', 'N/A')}\n"
                    f"Remediation: {audit_report.get('remediation', 'See audit report')}"
                )

        except QualityGateFailure:
            # Gate failed - result already recorded in manifest
            # Re-raise to block transition
            raise
        except Exception as e:
            # Unexpected error - record as ERROR status
            logger.error(f"Quality gate execution error: {e}")
            error_report = {
                "status": "ERROR",
                "message": str(e),
            }
            self._record_quality_gate_result(
                manifest=manifest,
                transition_name=transition_name,
                gate=gate,
                audit_report=error_report
            )

            # If blocking, propagate error
            if gate.get("blocking", False):
                raise QualityGateFailure(f"Quality gate execution failed: {e}") from e

    logger.info(f"‚úÖ Quality gates passed for: {transition_name}")
```

**Modify `invoke_auditor` to track duration (starting at line 878):**

Add at the beginning of the method:

```python
def invoke_auditor(
    self,
    check_type: str,
    manifest: ProjectManifest,
    severity: str = "info",
    blocking: bool = False,
) -> Dict[str, Any]:
    """
    Invoke AUDITOR agent for quality gate checks.

    NEW (GAD-004): Returns structured dict instead of just raising exceptions
    """
    start_time = time.time()  # Track duration

    logger.info(f"üîç Running {severity.upper()} audit: {check_type} (blocking={blocking})")

    # ... (existing code) ...
```

Add before the final return statement:

```python
    # Calculate duration
    duration_ms = int((time.time() - start_time) * 1000)

    audit_report = {
        "check_type": check_type,
        "severity": severity,
        "blocking": blocking,
        "status": status,
        "findings": findings,
        "timestamp": datetime.utcnow().isoformat() + "Z",
        "duration_ms": duration_ms  # NEW
    }

    # Add message and remediation if available
    if audit_result.get("message"):
        audit_report["message"] = audit_result["message"]
    if audit_result.get("remediation"):
        audit_report["remediation"] = audit_result["remediation"]

    # ... (existing blocking check code) ...

    return audit_report
```

---

### 4.3. Layer 3: Deployment-Scoped Validation

#### 4.3.1. Create GitHub Actions Workflow

**File:** `.github/workflows/post-merge-validation.yml`

```yaml
name: Post-Merge Validation

on:
  push:
    branches:
      - main
      - develop

env:
  PYTHON_VERSION: '3.11'

jobs:
  e2e-validation:
    name: E2E Tests & Validation
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install uv
        run: |
          curl -LsSf https://astral.sh/uv/install.sh | sh
          echo "$HOME/.cargo/bin" >> $GITHUB_PATH

      - name: Install dependencies
        run: |
          uv sync

      - name: Run E2E test suite
        id: e2e-tests
        run: |
          echo "Running E2E tests..."
          uv run pytest tests/e2e/ -v --tb=short --junitxml=e2e-results.xml

      - name: Run performance tests (non-blocking)
        id: perf-tests
        continue-on-error: true
        run: |
          echo "Running performance tests..."
          uv run pytest tests/performance/ -v --tb=short --junitxml=perf-results.xml || true

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-results
          path: |
            e2e-results.xml
            perf-results.xml

      - name: Report validation status
        if: always()
        uses: actions/github-script@v7
        with:
          script: |
            const e2eStatus = '${{ steps.e2e-tests.outcome }}';
            const perfStatus = '${{ steps.perf-tests.outcome }}';

            const status = e2eStatus === 'success' ? 'success' : 'failure';
            const message = e2eStatus === 'success'
              ? `‚úÖ Post-merge validation passed (E2E: ‚úÖ, Perf: ${perfStatus === 'success' ? '‚úÖ' : '‚ö†Ô∏è'})`
              : `‚ùå Post-merge validation failed (E2E: ‚ùå)`;

            await github.rest.repos.createCommitStatus({
              owner: context.repo.owner,
              repo: context.repo.repo,
              sha: context.sha,
              state: status,
              context: 'Post-Merge Validation',
              description: message,
              target_url: `${context.serverUrl}/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}`
            });

            core.info(`Validation complete: ${message}`);
```

---

## 5. HARNESS: Testing & Verification

### 5.1. Layer 1: Session-Scoped Enforcement Tests

#### 5.1.1. Manual Verification Checklist

**Test: System Status Linting Integration**

```bash
# 1. Update system status
./bin/update-system-status.sh

# 2. Verify .system_status.json contains linting status
cat .system_status.json | grep -A 3 '"linting"'

# Expected output:
#   "linting": {
#     "status": "passing",
#     "errors_count": 0
#   }

# 3. Verify show-context.sh displays linting status
./bin/show-context.sh | grep -i linting

# Expected output:
#   Linting: ‚úÖ Passing
```

**Test: Pre-Push Check Script**

```bash
# Test 1: Clean code (should pass)
./bin/pre-push-check.sh
# Expected: Exit code 0, "‚úÖ ALL PRE-PUSH CHECKS PASSED"

# Test 2: Introduce linting error
echo "import unused_module  # F401 error" >> test_file.py

./bin/pre-push-check.sh
# Expected: Exit code 1, "‚ùå PRE-PUSH CHECKS FAILED"

# Test 3: Fix and re-run
uv run ruff check . --fix
./bin/pre-push-check.sh
# Expected: Exit code 0, "‚úÖ ALL PRE-PUSH CHECKS PASSED"

# Cleanup
git restore test_file.py
```

**Test: Integration with git push**

```bash
# Test blocking behavior
echo "import unused  # Error" >> test.py
./bin/pre-push-check.sh && git push
# Expected: Script blocks, push never executes

# Fix and retry
uv run ruff check . --fix
./bin/pre-push-check.sh && git push
# Expected: Script passes, push executes
```

#### 5.1.2. Automated Test

**File:** `tests/test_session_enforcement.py` (new)

```python
#!/usr/bin/env python3
"""
Tests for Layer 1: Session-Scoped Enforcement
"""
import json
import subprocess
from pathlib import Path

def test_system_status_has_linting_field():
    """Verify .system_status.json contains linting status"""
    # Run update script
    result = subprocess.run(
        ["./bin/update-system-status.sh"],
        capture_output=True,
        text=True
    )
    assert result.returncode == 0, "update-system-status.sh failed"

    # Load status file
    status_file = Path(".system_status.json")
    assert status_file.exists(), ".system_status.json not found"

    with open(status_file) as f:
        status = json.load(f)

    # Verify linting field exists
    assert "linting" in status, "linting field missing from system status"
    assert "status" in status["linting"], "linting.status missing"
    assert status["linting"]["status"] in ["passing", "failing", "uv_not_available"]

    if status["linting"]["status"] == "failing":
        assert "errors_count" in status["linting"]
        assert status["linting"]["errors_count"] > 0

def test_pre_push_check_passes_on_clean_code():
    """Verify pre-push-check.sh passes when code is clean"""
    # First, ensure code is clean
    subprocess.run(["uv", "run", "ruff", "check", ".", "--fix"], check=True)
    subprocess.run(["uv", "run", "ruff", "format", "."], check=True)

    # Run pre-push check
    result = subprocess.run(
        ["./bin/pre-push-check.sh"],
        capture_output=True,
        text=True
    )

    assert result.returncode == 0, f"pre-push-check.sh failed on clean code:\n{result.stdout}"
    assert "‚úÖ ALL PRE-PUSH CHECKS PASSED" in result.stdout

def test_pre_push_check_fails_on_linting_errors():
    """Verify pre-push-check.sh fails when linting errors exist"""
    # Create temporary file with linting error
    test_file = Path("temp_test_linting.py")
    test_file.write_text("import unused_module  # F401 error\n")

    try:
        # Run pre-push check
        result = subprocess.run(
            ["./bin/pre-push-check.sh"],
            capture_output=True,
            text=True
        )

        assert result.returncode == 1, "pre-push-check.sh should have failed"
        assert "‚ùå PRE-PUSH CHECKS FAILED" in result.stdout
        assert "LINTING FAILED" in result.stdout

    finally:
        # Cleanup
        test_file.unlink(missing_ok=True)

if __name__ == "__main__":
    import sys

    print("Running Session Enforcement Tests...")

    try:
        test_system_status_has_linting_field()
        print("‚úÖ test_system_status_has_linting_field")

        test_pre_push_check_passes_on_clean_code()
        print("‚úÖ test_pre_push_check_passes_on_clean_code")

        test_pre_push_check_fails_on_linting_errors()
        print("‚úÖ test_pre_push_check_fails_on_linting_errors")

        print("\n‚úÖ ALL TESTS PASSED")
        sys.exit(0)

    except AssertionError as e:
        print(f"\n‚ùå TEST FAILED: {e}")
        sys.exit(1)
```

---

### 5.2. Layer 2: Workflow-Scoped Quality Gates Tests

#### 5.2.1. Manual Verification

**Test: Quality Gate Result Recording**

```bash
# 1. Run orchestrator with quality gates
python3 manual_planning_test.py

# 2. Check project manifest contains quality gate results
cat workspaces/manual-test-project/project_manifest.json | jq '.status.qualityGates'

# Expected output:
# {
#   "T1_StartCoding": {
#     "transition": "PLANNING.ARCHITECTURE_DESIGN ‚Üí CODING",
#     "gates": [
#       {
#         "check": "prompt_security_scan",
#         "status": "PASS",
#         "timestamp": "2025-11-16T...",
#         "duration_ms": 1234
#       }
#     ]
#   }
# }
```

#### 5.2.2. Automated Test

**File:** `tests/test_quality_gate_recording.py` (new)

```python
#!/usr/bin/env python3
"""
Tests for Layer 2: Workflow-Scoped Quality Gate Recording
"""
import json
import sys
from pathlib import Path
from unittest.mock import MagicMock, patch

# Add orchestrator to path
sys.path.insert(0, str(Path(__file__).parent.parent / "agency_os/core_system/orchestrator"))

from core_orchestrator import CoreOrchestrator, ProjectManifest, ProjectPhase

def test_quality_gate_result_recorded_in_manifest():
    """Verify quality gate results are written to manifest"""
    # Setup
    repo_root = Path(__file__).parent.parent
    orchestrator = CoreOrchestrator(repo_root=repo_root, execution_mode="delegated")

    # Create mock manifest
    manifest = ProjectManifest(
        project_id="test-project",
        name="Test Project",
        current_phase=ProjectPhase.PLANNING,
        metadata={"status": {}, "metadata": {"projectId": "test-project", "name": "Test"}}
    )

    # Mock gate config
    gate = {
        "check": "test_security_scan",
        "severity": "critical",
        "blocking": True
    }

    # Mock audit report
    audit_report = {
        "status": "PASS",
        "findings": 0,
        "message": "No security issues found",
        "remediation": "N/A",
        "duration_ms": 1500
    }

    # Record result
    orchestrator._record_quality_gate_result(
        manifest=manifest,
        transition_name="T1_TestTransition",
        gate=gate,
        audit_report=audit_report
    )

    # Verify manifest was updated
    assert "qualityGates" in manifest.metadata["status"]
    assert "T1_TestTransition" in manifest.metadata["status"]["qualityGates"]

    transition_record = manifest.metadata["status"]["qualityGates"]["T1_TestTransition"]
    assert "gates" in transition_record
    assert len(transition_record["gates"]) == 1

    gate_result = transition_record["gates"][0]
    assert gate_result["check"] == "test_security_scan"
    assert gate_result["status"] == "PASS"
    assert gate_result["severity"] == "critical"
    assert gate_result["blocking"] is True
    assert gate_result["findings"] == 0
    assert gate_result["duration_ms"] == 1500

    print("‚úÖ Quality gate result correctly recorded in manifest")

def test_multiple_gate_results_accumulated():
    """Verify multiple gate results are accumulated, not replaced"""
    repo_root = Path(__file__).parent.parent
    orchestrator = CoreOrchestrator(repo_root=repo_root)

    manifest = ProjectManifest(
        project_id="test-project",
        name="Test Project",
        current_phase=ProjectPhase.PLANNING,
        metadata={"status": {}, "metadata": {"projectId": "test-project", "name": "Test"}}
    )

    # Record first gate
    orchestrator._record_quality_gate_result(
        manifest=manifest,
        transition_name="T1_Test",
        gate={"check": "gate1", "blocking": True},
        audit_report={"status": "PASS", "findings": 0}
    )

    # Record second gate
    orchestrator._record_quality_gate_result(
        manifest=manifest,
        transition_name="T1_Test",
        gate={"check": "gate2", "blocking": False},
        audit_report={"status": "FAIL", "findings": 3}
    )

    # Verify both gates are recorded
    gates = manifest.metadata["status"]["qualityGates"]["T1_Test"]["gates"]
    assert len(gates) == 2
    assert gates[0]["check"] == "gate1"
    assert gates[1]["check"] == "gate2"

    print("‚úÖ Multiple gate results correctly accumulated")

if __name__ == "__main__":
    try:
        test_quality_gate_result_recorded_in_manifest()
        test_multiple_gate_results_accumulated()
        print("\n‚úÖ ALL QUALITY GATE RECORDING TESTS PASSED")
        sys.exit(0)
    except AssertionError as e:
        print(f"\n‚ùå TEST FAILED: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
```

---

### 5.3. Integration Test

**File:** `tests/test_multi_layer_integration.py` (new)

```python
#!/usr/bin/env python3
"""
Integration test for complete multi-layer quality enforcement system
"""
import json
import subprocess
from pathlib import Path

def test_complete_quality_enforcement_flow():
    """
    End-to-end test of all 3 layers working together
    """
    print("üß™ Testing complete multi-layer quality enforcement flow...")

    # LAYER 1: Session-scoped enforcement
    print("\n1Ô∏è‚É£  Testing Layer 1 (Session-Scoped Enforcement)...")

    # Ensure code is clean
    subprocess.run(["uv", "run", "ruff", "check", ".", "--fix"], check=True, capture_output=True)

    # Run pre-push check
    result = subprocess.run(["./bin/pre-push-check.sh"], capture_output=True, text=True)
    assert result.returncode == 0, "Layer 1: Pre-push check failed on clean code"
    print("   ‚úÖ Layer 1 passed")

    # Verify system status updated
    with open(".system_status.json") as f:
        status = json.load(f)
    assert "linting" in status
    assert status["linting"]["status"] == "passing"
    print("   ‚úÖ System status correctly updated")

    # LAYER 2: Workflow-scoped quality gates
    print("\n2Ô∏è‚É£  Testing Layer 2 (Workflow-Scoped Quality Gates)...")

    # This would run orchestrator with quality gates
    # For now, verify the code exists
    from core_orchestrator import CoreOrchestrator
    orch = CoreOrchestrator(repo_root=Path.cwd())
    assert hasattr(orch, "_record_quality_gate_result")
    assert hasattr(orch, "apply_quality_gates")
    print("   ‚úÖ Layer 2 code exists")

    # LAYER 3: Deployment-scoped validation
    print("\n3Ô∏è‚É£  Testing Layer 3 (Deployment-Scoped Validation)...")

    # Verify GitHub Actions workflow exists
    workflow_file = Path(".github/workflows/post-merge-validation.yml")
    assert workflow_file.exists(), "Layer 3: GitHub Actions workflow not found"
    print("   ‚úÖ Layer 3 workflow exists")

    print("\n‚úÖ ALL LAYERS INTEGRATED SUCCESSFULLY")

if __name__ == "__main__":
    try:
        test_complete_quality_enforcement_flow()
    except Exception as e:
        print(f"\n‚ùå INTEGRATION TEST FAILED: {e}")
        import traceback
        traceback.print_exc()
        exit(1)
```

---

### 5.4. Verification Checklist

**Before marking GAD-004 as complete, verify:**

- [ ] **Layer 1: Session-Scoped**
  - [ ] `bin/update-system-status.sh` includes linting check
  - [ ] `bin/show-context.sh` displays linting status
  - [ ] `bin/pre-push-check.sh` exists and is executable
  - [ ] Pre-push check blocks on linting failures
  - [ ] CLAUDE.md updated with new workflow
  - [ ] Manual test: `./bin/pre-push-check.sh` passes on clean code
  - [ ] Automated test: `tests/test_session_enforcement.py` passes

- [ ] **Layer 2: Workflow-Scoped**
  - [ ] `core_orchestrator.py` has `_record_quality_gate_result()` method
  - [ ] `apply_quality_gates()` calls recording before blocking
  - [ ] `invoke_auditor()` returns structured dict with duration
  - [ ] Manual test: Quality gate results appear in `project_manifest.json`
  - [ ] Automated test: `tests/test_quality_gate_recording.py` passes

- [ ] **Layer 3: Deployment-Scoped**
  - [ ] `.github/workflows/post-merge-validation.yml` exists
  - [ ] Workflow runs E2E tests on push to main
  - [ ] Workflow reports status via commit status API
  - [ ] Manual test: Trigger workflow and verify execution

- [ ] **Integration**
  - [ ] All 3 layers work independently
  - [ ] No conflicts between layers
  - [ ] Integration test passes: `tests/test_multi_layer_integration.py`
  - [ ] Session handoff updated to reflect completion

---

## 6. Rollout Plan

### 6.1. Phase 1: Layer 1 Implementation (Week 1)

**Goal:** Get session-scoped enforcement working

**Tasks:**
1. Extend `bin/update-system-status.sh` with linting/formatting checks
2. Extend `bin/show-context.sh` to display linting status
3. Create `bin/pre-push-check.sh`
4. Update CLAUDE.md
5. Write tests (`tests/test_session_enforcement.py`)
6. Manual testing
7. Commit & push

**Success Criteria:**
- Pre-push check blocks bad commits
- Agents see linting status in `./bin/show-context.sh`
- No CI/CD linting failures for 1 week

---

### 6.2. Phase 2: Layer 2 Implementation (Week 2)

**Goal:** Get workflow-scoped quality gates recording results

**Tasks:**
1. Add `_record_quality_gate_result()` to `core_orchestrator.py`
2. Add `_get_transition_config()` helper method
3. Modify `apply_quality_gates()` to record results
4. Modify `invoke_auditor()` to track duration
5. Write tests (`tests/test_quality_gate_recording.py`)
6. Manual testing with `manual_planning_test.py`
7. Commit & push

**Success Criteria:**
- Quality gate results appear in `project_manifest.json`
- Failed gates show actionable remediation messages
- Audit trail is complete and accurate

---

### 6.3. Phase 3: Layer 3 Implementation (Week 3)

**Goal:** Get post-merge validation working

**Tasks:**
1. Create `.github/workflows/post-merge-validation.yml`
2. Write E2E tests if needed
3. Test workflow on push to develop branch
4. Monitor for 1 week
5. Enable for main branch

**Success Criteria:**
- E2E tests run on every merge to main
- Results visible in GitHub commit status
- No false positives

---

### 6.4. Phase 4: Integration & Documentation (Week 4)

**Goal:** Polish and document complete system

**Tasks:**
1. Write integration test (`tests/test_multi_layer_integration.py`)
2. Update CLAUDE.md with full system overview
3. Create usage guide in `docs/guides/QUALITY_ENFORCEMENT_GUIDE.md`
4. Update session handoff
5. Mark GAD-004 as ‚úÖ Complete

**Success Criteria:**
- All tests pass
- Documentation complete
- System battle-tested for 1+ week
- No regressions

---

## 7. Success Metrics

### 7.1. Layer 1 Metrics

**Effectiveness:**
- CI/CD linting failures: **Target < 5% of pushes**
- Agent compliance: **Target 95%+ use pre-push-check.sh**
- False positive rate: **Target < 1%**

**Monitoring:**
```bash
# Count CI/CD linting failures in last 30 days
gh run list --workflow=validate.yml --limit 100 --json conclusion | \
  jq '[.[] | select(.conclusion == "failure")] | length'
```

### 7.2. Layer 2 Metrics

**Auditability:**
- Quality gate executions recorded: **Target 100%**
- Manifest completeness: **Target all transitions have gate records**

**Monitoring:**
```bash
# Count quality gate records in manifest
cat project_manifest.json | jq '.status.qualityGates | keys | length'
```

### 7.3. Layer 3 Metrics

**Production Readiness:**
- E2E test pass rate: **Target 95%+**
- Post-merge failures caught: **Target catch 100% of integration issues**

**Monitoring:**
```bash
# E2E test pass rate (last 30 days)
gh run list --workflow=post-merge-validation.yml --limit 100 --json conclusion | \
  jq '[.[] | select(.conclusion == "success")] | length'
```

---

## 8. Appendix: Rejected Alternatives

### 8.1. Gemini's "Inner Loop" Proposal

**Proposal:** Write linting results to `project_manifest.json`

**Rejection Reasons:**
1. **Scope confusion:** Linting is session-scoped (transient), not project-scoped (durable)
2. **Manifest pollution:** Multiple fix attempts bloat the file
3. **Wrong timing:** Orchestrator runs after commit (too late)
4. **Architectural mismatch:** Doesn't work with delegated execution

**What we took:** Runtime Feedback Loop concept for WORKFLOW-level gates

---

### 8.2. Git Hooks as Primary Enforcement

**Proposal:** Use `.githooks/pre-push` as primary enforcement

**Rejection Reasons:**
1. Requires user setup (`git config core.hooksPath .githooks`)
2. Doesn't work in CI/CD
3. Claude Code agents can't run git config
4. Unreliable across different environments

**What we did instead:** Optional git hooks + mandatory pre-push script

---

### 8.3. Orchestrator-Based Linting

**Proposal:** Add linting as AUDITOR check in workflow

**Rejection Reasons:**
1. Too late (code already committed)
2. Wrong scope (orchestrator manages projects, not commits)
3. Delegated mode mismatch (orchestrator is remote)

**What we did instead:** Session-scoped enforcement (Layer 1)

---

## 9. References

- **GAD-002:** Core SDLC Orchestration (quality gates foundation)
- **ADR-003:** Delegated Execution Architecture (why orchestrator is remote)
- **Session Handoff System:** `.session_handoff.json`, `bin/show-context.sh`
- **CLAUDE.md:** Operational truth (verification commands)

---

## 10. Document History

| Version | Date | Author | Changes |
|---------|------|--------|---------|
| 1.0 | 2025-11-16 | Claude Code (System Architect) | Initial version |

---

**STATUS: ‚úÖ Ready for Implementation**

**Next Steps:**
1. Review and approve GAD-004
2. Create session handoff for implementation
3. Begin Phase 1 (Layer 1 implementation)
