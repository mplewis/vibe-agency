# GAD-000: The Operator Inversion Principle

**Version:** 1.5
**Date:** 2025-11-21
**Status:** FOUNDATIONAL LAW
**Precedence:** HIGHEST - All subsequent GADs (1-9) must be interpreted through this lens
**Upgrade:** Added meta-insights, failure modes, boundary conditions, and evolutionary connection

---

## Preamble

**This is the Foundational Operating Principle of Vibe OS.**

All architecture decisions, design patterns, and implementation strategies in Vibe Agency must be viewed through this lens. This principle fundamentally changes how we think about:

- **User Experience Design** - Not for humans to operate, but for AI to operate on behalf of humans
- **Tool Interfaces** - Optimized for AI consumption, not human ergonomics
- **Documentation** - Written for AI discoverability, not human reading
- **Error Handling** - Machine-parseable, not human-friendly prose
- **State Management** - Always observable by AI operators

**GADs 1-9 are implementation details. GAD-000 is the philosophical foundation.**

---

## The Operator Inversion Principle

### Traditional Software Paradigm:

```
User operates the system
â”‚
â”œâ”€ User clicks buttons
â”œâ”€ User writes commands
â”œâ”€ User configures settings
â””â”€ System responds to user actions
```

### Prompt-as-Infrastructure Paradigm:

```
AI operates the system
â”‚
â”œâ”€ Human provides intent (natural language)
â”œâ”€ AI translates to operations
â”œâ”€ AI executes via system interfaces
â””â”€ Human validates outcomes (not operations)
```

---

## Why This Principle Emerges Now

### The GPT Era (2022-2024): Prompt Engineering

**Interaction Model:**
```
Human â†’ Crafts Perfect Prompt â†’ GPT â†’ One-Shot Response â†’ Human Evaluates
```

**Focus:** Prompt quality, temperature tuning, few-shot examples

**Result:** AI as **assistant** (human still does the work, AI helps)

---

### The Agentic Era (2024+): AI as Operator

**Interaction Model:**
```
Human â†’ Describes Goal â†’ AI Agent â†’ Multi-Step Execution â†’ Human Validates
                            â†“
                    [Uses Tools, Reads State, Self-Corrects]
```

**Focus:** Tool design, state observability, error recovery

**Result:** AI as **operator** (AI does the work, human directs strategy)

---

### The Critical Shift

| Era | Human's Role | AI's Role | Interface Design Priority |
|-----|--------------|-----------|---------------------------|
| **GPT Era** | Operator | Assistant | Human-friendly prompts |
| **Agentic Era** | Director | Operator | AI-parseable tools |

**GAD-000 exists because we are now in the Agentic Era.**

Systems that fail to make this shift will be:
- Hard for AI agents to operate
- Fragile (AI must scrape human-friendly text)
- Limited (AI cannot compose operations)
- Unreliable (AI cannot self-correct)

---

## What This Means for Architecture

### Traditional UX Design:

```yaml
question: "How will the user interact with this?"
focus:
  - Button placement
  - Menu structure
  - Form validation
  - Error messages
  - Keyboard shortcuts

assumption: "Human will directly manipulate the interface"
```

### AI-Native UX Design:

```yaml
question: "How will the AI interact with this on behalf of the user?"
focus:
  - Tool interfaces (function signatures)
  - State observability (can AI see what happened?)
  - Error parseability (can AI understand what failed?)
  - Idempotency (can AI safely retry?)
  - Composability (can AI chain operations?)

assumption: "AI will manipulate the interface, human will describe intent"
```

---

## Concrete Example

### Traditional CLI Design:

```bash
# Human types this:
$ prabhupada search --query "karma" --chapter 2 --format json --limit 10

# System requires human to know:
- Exact flag names
- Syntax rules
- Output format options
- Parameter constraints
```

### AI-Native CLI Design:

```bash
# Human says to Claude:
"Find verses about karma in Chapter 2"

# Claude translates to:
$ prabhupada search karma --chapter 2

# System is designed so AI can:
- Discover available commands (help output is structured)
- Understand errors (machine-readable error codes)
- Compose operations (output of one feeds input of next)
- Self-correct (retry with different parameters)
```

---

## The Architecture Implications

### GAD-8 (Integration Layer) Must Include:

**1. Tool Discoverability**

```yaml
# NOT: Man pages written for humans
# BUT: Machine-readable capability descriptions

tools:
  - name: "search"
    purpose: "Find verses matching criteria"
    parameters:
      - name: "query"
        type: "string"
        required: true
      - name: "chapter"
        type: "integer"
        optional: true
        range: [1, 18]
    returns:
      success: "array of verse objects"
      failure: "error code + description"
```

**2. State Transparency**

```python
# NOT: Silent internal state
# BUT: Observable system state

def get_system_status():
    """AI can query: What's the current state?"""
    return {
        "database_loaded": True,
        "verses_available": 613,
        "last_search": "karma",
        "current_chapter": 2
    }
```

**3. Composable Operations**

```bash
# AI can chain operations:
search karma | filter --chapter 2 | format json | save results.json

# System is designed for pipelines, not one-off commands
```

---

## The User Mental Model Shift

### Old Model (Human-Operated):

```
I am a user
â”‚
â”œâ”€ I learn the interface
â”œâ”€ I execute commands
â”œâ”€ I interpret results
â””â”€ I fix errors
```

### New Model (AI-Operated):

```
I am a director
â”‚
â”œâ”€ I describe goals
â”œâ”€ AI executes operations
â”œâ”€ AI interprets results
â”œâ”€ AI fixes errors
â””â”€ I validate outcomes
```

---

## For Vibe OS Specifically

### Current GAD Documents Assume:

```
"The agent (VIBE_ALIGNER) will interact with the user"
"The user provides input"
"The system returns output to the user"
```

### Should Actually Be:

```
"The agent (VIBE_ALIGNER) is operated BY an LLM (Claude)"
"The human provides intent to the LLM"
"The LLM operates the agent on behalf of the human"
"The human validates the LLM's execution"
```

---

## The GAD-8 Rewrite Needed

### Current Focus:

```yaml
GAD-8: Integration Matrix
focus: "How do agents talk to each other"
```

### Missing Focus:

```yaml
GAD-8: Integration Matrix
focus: "How does an LLM operate the entire system"

requirements:
  - Tool interfaces designed for AI execution
  - State always observable
  - Errors always parseable
  - Operations composable
  - Self-documentation for AI consumption
```

---

## Concrete Recommendations

### 1. Add GAD-8-AI: "LLM Operator Interface"

```yaml
purpose: "Define how an LLM interacts with Vibe OS"

principles:
  - discoverability: "LLM can learn available operations"
  - observability: "LLM can see system state"
  - parseability: "LLM can understand errors"
  - composability: "LLM can chain operations"
  - idempotency: "LLM can safely retry"

interfaces:
  - command_discovery: "list_available_tools()"
  - state_query: "get_system_status()"
  - operation_execution: "execute_tool(name, params)"
  - error_handling: "parse_error(error_code)"
  - result_validation: "validate_output(result, schema)"
```

### 2. Revise GAD-6 (Knowledge Department)

```yaml
# ADD: AI-readable knowledge schemas
knowledge_access:
  human_interface: âŒ "Not primary"
  ai_interface: âœ… "Primary design focus"

  operations:
    - query_knowledge(domain, question)
    - list_available_knowledge()
    - get_knowledge_schema(domain)
```

### 3. Revise GAD-7 (STEWARD)

```yaml
# ADD: AI governance
governance:
  human_mode: "Human reads governance docs, follows manually"
  ai_mode: "AI queries allowed_operations(), system enforces"

  operations:
    - check_permission(operation, context)
    - list_constraints(operation)
    - validate_action(proposed_action)
```

---

## The Fundamental Insight

**Traditional software:**
> "Design for humans to operate directly"

**AI-native software:**
> "Design for AI to operate, humans to direct"

**This changes EVERYTHING:**
- Documentation (for AI consumption)
- Error messages (machine-parseable)
- Interfaces (composable tools)
- State management (always observable)
- Testing (can AI successfully operate it?)

---

## The Message for Vibe OS Architect

```markdown
CRITICAL ARCHITECTURAL PRINCIPLE:

The end user is NOT the operator.
The LLM (Claude Code, etc.) is the operator.
The human is the director who provides intent.

Current GADs assume human-operated agents.
Reality: LLM-operated agents on behalf of humans.

This requires:
1. GAD-8 expansion: "LLM Operator Interface"
2. Tool design for AI consumption, not human UX
3. State transparency for AI observability
4. Error messages for AI parseability
5. Documentation for AI discoverability

The 6D model holds, but Layer 0 (interface design)
must assume AI operator, not human operator.

This is not a small change. This is a paradigm shift
that affects every GAD from 5-9.
```

---

## Implementation Checklist

Every tool, interface, and system component must answer:

- [ ] **Discoverability**: Can an AI discover this tool exists?
- [ ] **Observability**: Can an AI see the current state?
- [ ] **Parseability**: Can an AI understand errors?
- [ ] **Composability**: Can an AI chain this with other operations?
- [ ] **Idempotency**: Can an AI safely retry this operation?
- [ ] **Documentation**: Is documentation AI-readable (structured)?

**If any answer is "no", the design is not AI-native.**

---

## Validation Examples

### âœ… GOOD: AI-Native Design

```python
# bin/vibe status
{
  "health": {
    "git_status": {"status": "clean", "changes": 0},
    "vibe_cli": {"available": true, "path": "./vibe-cli"},
    "cartridges": {"available": 3, "loaded": ["feature-implement", "coder-mode", "hello-world"]}
  },
  "next_steps": [
    {"command": "vibe run [theme]", "purpose": "Launch cartridge"},
    {"command": "vibe make [wish]", "purpose": "Execute feature"}
  ]
}
```

**Why good?**
- AI can parse JSON
- AI can see system state
- AI can discover available commands
- AI can chain operations

### âŒ BAD: Human-Native Design

```bash
# bin/vibe status
ðŸŸ¢ VIBE AGENCY - SYSTEM STATUS
================================
âœ… Git Status: Clean
âœ… Cartridges: 3 available
âš™ï¸  Next Steps: Run 'vibe --help'
```

**Why bad?**
- Emoji decorations (not parseable)
- Human-friendly prose (ambiguous)
- No structured data
- AI must scrape text (error-prone)

---

## What Happens If You Ignore GAD-000

### Anti-Pattern 1: Human-Friendly Errors

**Violation:**
```python
raise Exception("Oops! Something went wrong. Please try again later.")
```

**Why It Fails AI:**
- No error code (AI cannot categorize)
- No context (AI cannot diagnose)
- Vague instruction (AI cannot self-correct)

**GAD-000 Compliant:**
```python
raise ToolExecutionError(
    code="TOOL_TIMEOUT",
    message="Tool execution exceeded 30s timeout",
    context={
        "tool": "search",
        "timeout_ms": 30000,
        "elapsed_ms": 31250
    },
    retry_strategy="exponential_backoff"
)
```

**Why It Works:**
- AI can parse error code
- AI understands what failed
- AI knows how to retry

---

### Anti-Pattern 2: Hidden State

**Violation:**
```python
# Internal state not exposed
class Database:
    def __init__(self):
        self._connection = None  # Hidden!
        self._query_count = 0     # Hidden!
```

**Why It Fails AI:**
- AI cannot see if database is connected
- AI cannot monitor query quotas
- AI must guess system state

**GAD-000 Compliant:**
```python
class Database:
    def get_status(self) -> dict:
        """AI-observable state"""
        return {
            "connected": self._connection is not None,
            "query_count": self._query_count,
            "quota_remaining": 1000 - self._query_count,
            "last_query_time": self._last_query.isoformat()
        }
```

**Why It Works:**
- AI can query current state
- AI can make informed decisions
- AI can avoid quota violations

---

### Anti-Pattern 3: Non-Discoverable Tools

**Violation:**
```bash
# help output designed for humans
$ mytool --help
MyTool - The Best Tool Ever!
Usage: mytool [options]
  -f    Do the thing
  -v    Verbose mode
  ...
```

**Why It Fails AI:**
- AI must parse English prose
- No machine-readable schema
- Cannot programmatically discover capabilities

**GAD-000 Compliant:**
```bash
$ mytool --help --json
{
  "name": "mytool",
  "version": "1.0.0",
  "commands": [
    {
      "name": "search",
      "purpose": "Search database",
      "parameters": [
        {"name": "query", "type": "string", "required": true},
        {"name": "limit", "type": "integer", "default": 10}
      ],
      "returns": {"type": "array", "items": "SearchResult"}
    }
  ]
}
```

**Why It Works:**
- AI can parse tool capabilities
- AI understands parameter types
- AI can generate correct invocations

---

### Anti-Pattern 4: One-Off Commands

**Violation:**
```bash
# Cannot compose
$ generate-report --output report.txt
$ send-email --attach report.txt
```

**Why It Fails AI:**
- AI must coordinate file paths manually
- No guarantee intermediate files exist
- Fragile if filesystem changes

**GAD-000 Compliant:**
```bash
# Composable pipeline
$ generate-report | send-email --stdin
```

**Why It Works:**
- AI chains operations naturally
- No intermediate file management
- Single command (atomic)

---

### The Cost of Non-Compliance

**When you violate GAD-000:**

1. **AI agents cannot operate your system** (requires human mediation)
2. **Fragility increases** (text scraping breaks on UI changes)
3. **Scalability suffers** (cannot automate multi-step workflows)
4. **Reliability decreases** (AI cannot self-correct errors)
5. **Integration fails** (other AI systems cannot interoperate)

**Result:** Your system remains in the "GPT Era" (AI as assistant, not operator)

---

## When GAD-000 Does NOT Apply

### Boundary Conditions

**GAD-000 applies to:**
- âœ… **System Interfaces** (Tools, APIs, CLIs)
- âœ… **Error Reporting** (Codes, context, recovery strategies)
- âœ… **State Management** (Observable, queryable state)
- âœ… **Documentation** (Machine-readable schemas)
- âœ… **Workflow Design** (Composable, chainable operations)

**GAD-000 does NOT apply to:**
- âŒ **Strategic Decisions** (Human judgment on business priorities)
- âŒ **Ethical Governance** (Human oversight of AI actions)
- âŒ **Creative Direction** (Human vision and intent)
- âŒ **Final Approval** (Human validation of outcomes)
- âŒ **User-Facing Content** (Customer-visible text, marketing copy)

---

### The Human-in-the-Loop Principle

**GAD-000 is NOT about removing humans. It's about correct role assignment:**

| Decision Type | Who Decides | Interface Type |
|---------------|-------------|----------------|
| **What to build** | Human (Director) | Natural language intent |
| **How to build it** | AI (Operator) | Structured tools/APIs |
| **Is it correct?** | Human (Validator) | Human-readable summaries |

**Example:**
```
Human: "Create a landing page for our product"  (WHAT - Strategy)
AI:    Executes via tools, reads state, self-corrects  (HOW - Execution)
Human: "Yes, ship it" or "Change the headline"  (VALIDATION - Judgment)
```

**GAD-000 optimizes the HOW layer (AI execution), not the WHAT or VALIDATION layers.**

---

## The Evolutionary Connection (EAD)

### The Feedback Loop: How Systems Learn

**Traditional Software:**
```
Build â†’ Ship â†’ User Feedback â†’ Manual Updates â†’ Build (v2)
```

**AI-Native Software (Vibe OS + EAD):**
```
Build â†’ Ship â†’ AI Operator Feedback â†’ Automated Learning â†’ Evolve
           â†“                              â†“
    [Usage Patterns]              [Self-Improving Tools]
           â†“                              â†“
    [Error Patterns]              [Better Interfaces]
```

---

### The Evolutionary Architecture Dimension (EAD)

**What is EAD?**

EAD (Evolutionary Architecture Dimension) is the **6th dimension** of the 6D Hexagon:

| Dimension | Name | Function | GAD-000 Relationship |
|-----------|------|----------|----------------------|
| **1-3D** | GAD/LAD/VAD | Structure | AI operates these |
| **4D** | PAD | Workflows | AI executes these |
| **5D** | MAD | Context | AI consumes this |
| **6D** | **EAD** | Evolution | AI improves system based on usage |

**How EAD Depends on GAD-000:**

```yaml
without_gad_000:
  evolution: "Human analyzes logs, manually improves system"
  speed: "Weeks to months"
  scalability: "Limited by human bandwidth"

with_gad_000:
  evolution: "AI analyzes structured logs, proposes improvements"
  speed: "Hours to days"
  scalability: "Unlimited (AI can process all usage data)"
```

---

### The Self-Improvement Loop

**Example: Tool Discovery Evolution**

1. **Week 1:** AI agents struggle to find `bin/vibe-knowledge` tool
2. **GAD-000 Compliance:** Tool outputs usage stats in JSON
   ```json
   {"tool": "vibe-knowledge", "discovery_failures": 47, "success_rate": 0.23}
   ```
3. **EAD Analysis:** AI detects low discoverability
4. **EAD Proposal:** Add tool to `bin/vibe status --json` output under "available_tools"
5. **Human Approval:** Director validates change
6. **Auto-Deployment:** System evolves

**Without GAD-000:** This loop is manual (weeks)
**With GAD-000:** This loop is automated (days)

---

### Why EAD Cannot Exist Without GAD-000

**The Dependency:**
```
EAD requires observable system behavior
   â†“
Observable behavior requires structured data
   â†“
Structured data requires AI-native interfaces
   â†“
AI-native interfaces require GAD-000
```

**Conclusion:** GAD-000 is the **prerequisite** for EAD.

---

## The GAD-000 Turing Test

**Can an AI successfully operate your system without human intervention?**

### Test Questions

Ask these about every tool, interface, and component:

1. **Discoverability Test**
   - Can AI find this tool without being told?
   - Is there a `--help --json` or equivalent?
   - Are capabilities machine-readable?

2. **Observability Test**
   - Can AI query current system state?
   - Is state exposed in structured format?
   - Can AI detect errors before they cascade?

3. **Parseability Test**
   - Are errors machine-readable (error codes + context)?
   - Can AI distinguish transient vs permanent failures?
   - Does AI know how to retry?

4. **Composability Test**
   - Can AI chain this tool with others?
   - Does output match input schemas of dependent tools?
   - Can AI build pipelines programmatically?

5. **Idempotency Test**
   - Can AI safely retry this operation?
   - Does the tool report "already done" vs "failed"?
   - Is state updated atomically?

### Scoring

**Pass:** All 5 tests answered "Yes" â†’ GAD-000 Compliant âœ…
**Partial:** 3-4 tests answered "Yes" â†’ Needs improvement âš ï¸
**Fail:** â‰¤2 tests answered "Yes" â†’ Violates GAD-000 âŒ

---

## Conclusion

**GAD-000 is the lens through which all other GADs must be viewed.**

When designing any component of Vibe OS, ask:

> "Is this designed for an AI to operate on behalf of a human?"

If the answer is no, the design violates GAD-000.

---

## Related GADs

- **GAD-005**: Pre-Action Kernel (Safety for AI operators)
- **GAD-006**: Tool Safety Guard (Capability-based security for AI)
- **GAD-008**: Integration Matrix (Must include LLM operator interface)
- **All GADs**: Reinterpret through Operator Inversion lens

---

## Practical Tooling: Enforcing GAD-000 Compliance

This section provides copy-paste code and scripts for developers to enforce GAD-000 compliance in their projects.

### Pre-Commit Hook: Validate Tool Output

**File: `.git/hooks/pre-commit`**

```bash
#!/bin/bash
# GAD-000 Compliance Check: Ensure tools output machine-readable formats

set -e

echo "ðŸ” GAD-000 Compliance Check"

# Check for common violations
violations=0

# 1. Find tools without --help --json support
echo "Checking for structured help output..."
for tool in bin/vibe-*; do
    if [ -x "$tool" ]; then
        if ! "$tool" --help --json 2>/dev/null | jq . > /dev/null 2>&1; then
            echo "âš ï¸  $tool: Missing --help --json support"
            violations=$((violations + 1))
        fi
    fi
done

# 2. Find error messages without error codes
echo "Checking for structured error handling..."
if grep -r "raise Exception\|raise Error" src/ --include="*.py" 2>/dev/null | grep -v "ErrorCode\|ToolExecutionError"; then
    echo "âš ï¸  Found unstructured error handling (missing error codes)"
    violations=$((violations + 1))
fi

# 3. Find tools without status methods
echo "Checking for observable state..."
if grep -r "def get_status\|def status" src/ --include="*.py" 2>/dev/null | wc -l | grep -q "0"; then
    echo "âš ï¸  No observable state methods found"
    violations=$((violations + 1))
fi

if [ $violations -gt 0 ]; then
    echo "âŒ GAD-000 violations detected ($violations)"
    exit 1
else
    echo "âœ… GAD-000 compliance check passed"
    exit 0
fi
```

### CI/CD Check: GitHub Actions

**File: `.github/workflows/gad-000-check.yml`**

```yaml
name: GAD-000 Compliance

on: [pull_request]

jobs:
  gad-000-check:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Check Tool Discoverability
        run: |
          echo "Verifying all tools have --help --json output..."
          for tool in bin/vibe-*; do
              if [ -x "$tool" ]; then
                  $tool --help --json | jq . > /dev/null || {
                      echo "âŒ $tool missing structured help"
                      exit 1
                  }
              fi
          done

      - name: Check Error Parseability
        run: |
          echo "Verifying error handling includes error codes..."
          ! grep -r "raise Exception" src/ --include="*.py" | grep -v "ErrorCode" || exit 1

      - name: Check State Observability
        run: |
          echo "Verifying components expose state..."
          grep -r "def.*status\|def.*state" src/ --include="*.py" || {
              echo "âš ï¸  Warning: Limited state exposure found"
          }
```

### Runtime Monitoring: Python Instrumentation

**File: `vibe/monitoring/gad000_monitor.py`**

```python
import time
import json
from functools import wraps
from typing import Callable, Dict, Any
from vibe.monitoring.gad000_metrics import GAD000Metrics

class GAD000Monitor:
    """Decorator to monitor GAD-000 compliance at runtime"""

    def __init__(self):
        self.tools_metrics: Dict[str, GAD000Metrics] = {}

    def monitor_tool(self, tool_name: str) -> Callable:
        """Decorator: Monitor tool execution for GAD-000 compliance"""
        if tool_name not in self.tools_metrics:
            self.tools_metrics[tool_name] = GAD000Metrics(tool_name)

        metrics = self.tools_metrics[tool_name]

        def decorator(func: Callable) -> Callable:
            @wraps(func)
            def wrapper(*args, **kwargs) -> Any:
                # Time execution
                start_ms = int(time.time() * 1000)

                try:
                    # Execute tool
                    result = func(*args, **kwargs)

                    # Record success
                    elapsed_ms = int(time.time() * 1000) - start_ms
                    metrics.record_execution(success=True, time_ms=elapsed_ms)

                    return result

                except Exception as e:
                    # Record failure with structured error
                    elapsed_ms = int(time.time() * 1000) - start_ms
                    error_code = getattr(e, "code", type(e).__name__)
                    metrics.record_execution(success=False, time_ms=elapsed_ms, error_code=error_code)
                    raise

            return wrapper
        return decorator

    def get_compliance_report(self, tool_name: str) -> Dict:
        """Get GAD-000 compliance metrics for a tool"""
        if tool_name not in self.tools_metrics:
            return {"error": "No metrics for tool"}

        metrics = self.tools_metrics[tool_name]
        aggregated = metrics.get_aggregated_metrics()

        return {
            "tool": tool_name,
            "compliance": {
                "discoverability": aggregated.get("discoverability_rate", 0) >= 0.95,
                "parseability": aggregated.get("success_rate", 0) >= 0.90,
                "idempotency": aggregated.get("self_correction_rate", 0) >= 0.70
            },
            "metrics": aggregated
        }
```

### CLI Tool: Check Compliance

**File: `bin/vibe gad000 check --tool <name>`**

```bash
#!/usr/bin/env python3
"""Check GAD-000 compliance for a tool"""

import sys
import json
import subprocess
from pathlib import Path

def check_tool_discoverability(tool_name: str) -> bool:
    """Verify tool has --help --json"""
    try:
        result = subprocess.run(
            [f"bin/{tool_name}", "--help", "--json"],
            capture_output=True,
            timeout=5
        )
        json.loads(result.stdout)
        return True
    except:
        return False

def check_error_parseability(tool_name: str) -> bool:
    """Verify errors include error codes"""
    tool_path = Path(f"bin/{tool_name}")
    if tool_path.suffix == ".py":
        content = tool_path.read_text()
        has_error_codes = "ErrorCode" in content or "error_code" in content
        return has_error_codes
    return True  # Skip non-Python tools

def check_state_observability(tool_name: str) -> bool:
    """Verify tool exposes state"""
    try:
        result = subprocess.run(
            [f"bin/{tool_name}", "status", "--json"],
            capture_output=True,
            timeout=5
        )
        json.loads(result.stdout)
        return True
    except:
        return False

def main():
    tool_name = sys.argv[1] if len(sys.argv) > 1 else "vibe"

    checks = {
        "discoverability": check_tool_discoverability(tool_name),
        "parseability": check_error_parseability(tool_name),
        "observability": check_state_observability(tool_name),
    }

    passed = sum(checks.values())
    total = len(checks)

    print(json.dumps({
        "tool": tool_name,
        "checks": checks,
        "passed": passed,
        "total": total,
        "compliant": passed >= 2  # At least 2 of 3
    }, indent=2))

    sys.exit(0 if passed >= 2 else 1)

if __name__ == "__main__":
    main()
```

---

## The Meta-Insight: Prompts ARE Infrastructure

### The Paradigm Shift

**Traditional Software Engineering:**
```
Infrastructure = Servers, Databases, APIs, Networks
Code = The product we build on infrastructure
```

**AI-Native Software Engineering:**
```
Infrastructure = Servers, Databases, APIs, Networks, PROMPTS
Code = Tools that AI operates via prompts
```

---

### Prompts as Load-Bearing Architecture

**In the Agentic Era:**

- **Prompts are not ephemeral** (they are durable system components)
- **Prompts are not user-facing** (they are AI-facing interfaces)
- **Prompts are not documentation** (they are executable specifications)

**Prompts are the interface layer between human intent and AI execution.**

---

### The Stack Redefined

**Traditional Stack:**
```
Layer 7: User Interface (HTML, CSS, JS)
Layer 6: Application Logic (Python, Java, etc.)
Layer 5: API Layer (REST, GraphQL)
Layer 4: Data Layer (SQL, NoSQL)
Layer 3: Infrastructure (Servers, Containers)
Layer 2: Network (TCP/IP, DNS)
Layer 1: Hardware (CPUs, Memory)
```

**AI-Native Stack:**
```
Layer 8: Human Intent (Natural Language)      â† NEW
Layer 7: Prompt Infrastructure (AI Operators)  â† NEW
Layer 6: Tool Layer (APIs optimized for AI)    â† CHANGED (AI-parseable)
Layer 5: State Layer (Always Observable)       â† CHANGED (AI-readable)
Layer 4: Data Layer (SQL, NoSQL)
Layer 3: Infrastructure (Servers, Containers)
Layer 2: Network (TCP/IP, DNS)
Layer 1: Hardware (CPUs, Memory)
```

**Key Changes:**
- **Layer 8 added:** Human provides intent, not operations
- **Layer 7 added:** AI translates intent â†’ operations
- **Layer 6 changed:** APIs must be AI-discoverable and composable
- **Layer 5 changed:** State must be AI-observable

---

### Why This Matters

**Traditional DevOps:**
```
Manage: Servers, databases, APIs, deployments
Monitor: Uptime, latency, errors, traffic
Optimize: Performance, cost, reliability
```

**AI-Native DevOps:**
```
Manage: Servers, databases, APIs, deployments, PROMPTS
Monitor: Uptime, latency, errors, traffic, AI SUCCESS RATE
Optimize: Performance, cost, reliability, AI OPERABILITY
```

**New Metrics:**
- **AI Discoverability Rate:** Can AI find the tool? (target: 95%+)
- **AI Success Rate:** Can AI execute without human help? (target: 90%+)
- **AI Self-Correction Rate:** Can AI recover from errors? (target: 80%+)
- **Tool Composability Score:** Can AI chain operations? (target: 100%)

### Metrics Schema & Instrumentation

**JSON Schema for Monitoring Dashboard:**
```json
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "type": "object",
  "properties": {
    "timestamp": {"type": "string", "format": "date-time"},
    "tool_name": {"type": "string"},
    "ai_metrics": {
      "type": "object",
      "properties": {
        "discovery_success": {"type": "boolean"},
        "discovery_time_ms": {"type": "integer"},
        "execution_success": {"type": "boolean"},
        "execution_time_ms": {"type": "integer"},
        "self_correction_attempts": {"type": "integer"},
        "error_code": {"type": ["string", "null"]},
        "composability_chain_length": {"type": "integer"}
      }
    },
    "aggregated": {
      "type": "object",
      "properties": {
        "discoverability_rate": {"type": "number", "minimum": 0, "maximum": 1},
        "success_rate": {"type": "number", "minimum": 0, "maximum": 1},
        "self_correction_rate": {"type": "number", "minimum": 0, "maximum": 1},
        "composability_score": {"type": "number", "minimum": 0, "maximum": 1}
      }
    }
  }
}
```

**Python Instrumentation Class:**
```python
import json
from datetime import datetime
from typing import Dict, Optional

class GAD000Metrics:
    """Track AI operator compliance metrics"""

    def __init__(self, tool_name: str):
        self.tool_name = tool_name
        self.metrics = []

    def record_discovery(self, success: bool, time_ms: int):
        """Record tool discovery attempt"""
        self.metrics.append({
            "timestamp": datetime.utcnow().isoformat(),
            "event": "discovery",
            "success": success,
            "time_ms": time_ms
        })

    def record_execution(self, success: bool, time_ms: int, error_code: Optional[str] = None):
        """Record tool execution attempt"""
        self.metrics.append({
            "timestamp": datetime.utcnow().isoformat(),
            "event": "execution",
            "success": success,
            "time_ms": time_ms,
            "error_code": error_code
        })

    def record_self_correction(self, attempt_num: int, strategy: str):
        """Record AI self-correction attempt"""
        self.metrics.append({
            "timestamp": datetime.utcnow().isoformat(),
            "event": "self_correction",
            "attempt": attempt_num,
            "strategy": strategy
        })

    def get_aggregated_metrics(self) -> Dict:
        """Calculate aggregated success rates"""
        if not self.metrics:
            return {}

        discoveries = [m for m in self.metrics if m["event"] == "discovery"]
        executions = [m for m in self.metrics if m["event"] == "execution"]
        corrections = [m for m in self.metrics if m["event"] == "self_correction"]

        return {
            "tool_name": self.tool_name,
            "discoverability_rate": sum(1 for m in discoveries if m["success"]) / len(discoveries) if discoveries else 0,
            "success_rate": sum(1 for m in executions if m["success"]) / len(executions) if executions else 0,
            "self_correction_rate": len(corrections) / len(executions) if executions else 0,
            "sample_count": len(self.metrics)
        }

    def export_json(self) -> str:
        """Export metrics for dashboard ingestion"""
        return json.dumps({
            "tool": self.tool_name,
            "metrics": self.metrics,
            "aggregated": self.get_aggregated_metrics()
        }, default=str)
```

**Alerting Rules (YAML):**
```yaml
alert_rules:
  - name: "Low AI Discoverability"
    condition: "discoverability_rate < 0.95"
    severity: "warning"
    action: "Review tool help output and schema"

  - name: "High AI Failure Rate"
    condition: "success_rate < 0.90"
    severity: "critical"
    action: "Investigate error codes and output parseability"

  - name: "Degraded Self-Correction"
    condition: "self_correction_rate < 0.70"
    severity: "warning"
    action: "Review tool idempotency and error recovery"
```

---

### The Engineering Implications

**1. Prompts Require Version Control**
```bash
# Traditional
git commit -m "Update API endpoint"

# AI-Native
git commit -m "Update API endpoint + AI operator prompt"
```

**2. Prompts Require Testing**
```python
# Test that AI can discover and use the tool
def test_ai_discoverability():
    tools = get_available_tools(format="json")
    assert "search" in [t["name"] for t in tools]

def test_ai_execution():
    result = ai_agent.execute("search for karma")
    assert result.success == True
```

**3. Prompts Require Monitoring**
```python
# Track AI operator success rates
metrics.track("ai_tool_discovery", {
    "tool": "search",
    "discovered": True,
    "time_to_discover_ms": 250
})
```

**4. Prompts Require Documentation**
```yaml
# Not just human docs, but machine-readable schemas
tool: "search"
ai_interface:
  discovery: "Available via --help --json"
  invocation: "search [query] [--flags]"
  output_format: "JSON array of results"
  error_codes: ["QUERY_EMPTY", "TIMEOUT", "RATE_LIMIT"]
```

---

### The Ultimate Realization

**Software engineering in the Agentic Era requires:**

1. **Designing for AI operators** (not human users)
2. **Treating prompts as infrastructure** (not throwaway text)
3. **Monitoring AI success rates** (not just uptime)
4. **Versioning AI interfaces** (not just code)
5. **Testing AI operability** (not just functionality)

**GAD-000 is the acknowledgment that the AI is now part of your stack.**

---

### The Future

**Today (2024):**
- AI operates tools designed for humans (fragile, error-prone)
- Humans mediate when AI fails (bottleneck)

**Tomorrow (2025+):**
- Tools designed for AI operation (robust, self-correcting)
- AI operates autonomously (humans direct strategy only)

**GAD-000 is the bridge from Today to Tomorrow.**

---

**END OF GAD-000**

*This document establishes the foundational operating principle. All subsequent architecture decisions flow from this.*

**Version History:**
- **v1.0** (2025-11-21): Initial codification
- **v1.5** (2025-11-21): Added meta-insights, failure modes, boundary conditions, EAD connection, Turing Test, and infrastructure paradigm
- **v1.6** (2025-11-22): Enhanced metrics with JSON schema and instrumentation code; added Practical Tooling section with pre-commit hooks, CI/CD checks, runtime monitoring, and CLI compliance checker
