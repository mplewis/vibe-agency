# GAD-511: Neural Adapter Strategy - Multi-Provider LLM Support

**Status:** âœ… IMPLEMENTED
**Date:** 2025-11-18
**Pillar:** GAD-5XX (Runtime Engineering)
**Priority:** P1 (Provider Independence & Flexibility)

---

## Executive Summary

**GAD-511** implements the **Neural Adapter Strategy Pattern** to enable provider-agnostic LLM integration, allowing VIBE Agency OS to work with multiple AI providers (Anthropic, Google, OpenAI, Local) through a uniform interface.

**The Problem:** Without provider abstraction:
- System locked to single LLM provider (vendor lock-in)
- Cannot leverage multiple providers for cost optimization
- Difficult to switch providers during outages
- No graceful degradation when provider unavailable

**The Solution:** Strategy Pattern with:
- **Abstract Provider Interface:** Uniform API across all providers
- **Concrete Providers:** Anthropic (Claude), Google (Gemini), OpenAI (GPT), Local (Ollama)
- **Provider Factory:** Auto-detection based on available API keys
- **NoOp Fallback:** Graceful degradation to knowledge-only mode

**Impact:** System gains provider independence, cost flexibility, and resilience against single-provider outages.

---

## Architecture

### Strategy Pattern Structure

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         LLMClient (Context)                 â”‚
â”‚  - Uses provider abstraction                â”‚
â”‚  - Adds safety layer (CB + Quota)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚ uses
                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      LLMProvider (Abstract Interface)       â”‚
â”‚  + invoke(prompt, model, ...) â†’ Response   â”‚
â”‚  + calculate_cost(tokens, model) â†’ float   â”‚
â”‚  + get_available_models() â†’ list[str]      â”‚
â”‚  + is_available() â†’ bool                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â†“                 â†“          â†“         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ AnthropicProviderâ”‚ â”‚  Google  â”‚ â”‚ OpenAI  â”‚ â”‚   NoOp   â”‚
â”‚    (Claude)     â”‚ â”‚ (Gemini) â”‚ â”‚  (GPT)  â”‚ â”‚ (Mock)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                  Factory (Auto-Detection)
                           â†“
            Priority: Google â†’ Anthropic â†’ OpenAI â†’ NoOp
```

### Implementation Location

**Provider System:**
```
agency_os/core_system/runtime/providers/
â”œâ”€â”€ __init__.py           # Public API exports
â”œâ”€â”€ base.py               # Abstract interface + NoOp
â”œâ”€â”€ factory.py            # Provider creation & auto-detection
â”œâ”€â”€ anthropic.py          # Claude provider
â””â”€â”€ google.py             # Gemini provider
```

**Integration:**
```
agency_os/core_system/runtime/llm_client.py   # LLMClient uses providers
agency_os/config/phoenix.py                 # Model configuration
```

**Key Classes:**
- `LLMProvider` (base.py) - Abstract interface
- `AnthropicProvider` - Claude 3.5 Sonnet implementation
- `GoogleProvider` - Gemini 2.5 Flash implementation
- `NoOpProvider` - Fallback mock provider
- `create_provider()` - Factory function
- `get_default_provider()` - Auto-detection

---

## Key Decisions

### Decision 1: Strategy Pattern (Not Adapter Pattern)

**Choice:** Strategy Pattern with provider swapping
**Rationale:**
- Need to swap providers at runtime (failover, cost optimization)
- Each provider has different capabilities and pricing
- Want uniform interface but different implementations
- Factory pattern for auto-selection

**Alternative Rejected:** Adapter Pattern
- Too rigid for multi-provider scenarios
- Doesn't support runtime switching

### Decision 2: Auto-Detection via API Keys

**Choice:** Detect available provider by checking environment variables
**Rationale:**
- Zero-config experience (just set API key)
- Graceful fallback chain
- Explicit priority order (Google â†’ Anthropic â†’ OpenAI â†’ NoOp)

**Detection Logic:**
```python
Priority Order:
1. GOOGLE_API_KEY â†’ GoogleProvider (Gemini)
2. ANTHROPIC_API_KEY â†’ AnthropicProvider (Claude)
3. OPENAI_API_KEY â†’ OpenAIProvider (GPT) [Future]
4. None available â†’ NoOpProvider (Mock)
```

### Decision 3: NoOp Provider for Graceful Degradation

**Choice:** Fallback to NoOpProvider when no API keys available
**Rationale:**
- System doesn't crash without API keys
- Enables knowledge-only mode (local reasoning)
- Better developer experience (can explore system without API)
- Prevents silent failures

**NoOp Behavior:**
```python
# Returns mock responses with zero cost
response = NoOpProvider().invoke(prompt="...")
# Result: content="{}", tokens=0, cost=$0.00
```

### Decision 4: Provider-Specific Cost Calculation

**Choice:** Each provider calculates its own costs
**Rationale:**
- Pricing models differ significantly across providers
- Input/output token ratios vary
- Enables accurate cost tracking per provider

**Examples:**
```python
# Anthropic (Claude 3.5 Sonnet)
input_cost = (tokens / 1M) * $3.00
output_cost = (tokens / 1M) * $15.00

# Google (Gemini 2.5 Flash - Experimental)
input_cost = (tokens / 1M) * $0.00   # Free tier
output_cost = (tokens / 1M) * $0.00   # Free tier
```

---

## Implementation Status

### âœ… Implemented

**Provider System:**
- âœ… `LLMProvider` abstract base class (base.py)
- âœ… `AnthropicProvider` for Claude (anthropic.py)
- âœ… `GoogleProvider` for Gemini (google.py)
- âœ… `NoOpProvider` fallback (base.py)
- âœ… Provider factory with auto-detection (factory.py)

**Integration:**
- âœ… `LLMClient` uses provider system (llm_client.py:235-248)
- âœ… Phoenix Config model settings (phoenix.py:177-215)
- âœ… Backward compatibility with legacy NoOpClient

**Features:**
- âœ… Uniform `invoke()` interface across providers
- âœ… Provider-specific cost calculation
- âœ… API key auto-loading from environment
- âœ… Graceful fallback to NoOp
- âœ… Standardized response format

### âš ï¸ Partially Implemented

**OpenAI Provider:**
- âš ï¸ Factory recognizes `OPENAI_API_KEY` but returns NoOp
- ðŸ“‹ Full implementation deferred (GAD-511 Phase 2)

**Local/Ollama Provider:**
- âš ï¸ Factory recognizes local option but returns NoOp
- ðŸ“‹ Full implementation deferred (GAD-511 Phase 2)

### Verification Commands

```bash
# Check provider implementation
ls -la agency_os/core_system/runtime/providers/

# Read implementations
cat agency_os/core_system/runtime/providers/base.py
cat agency_os/core_system/runtime/providers/anthropic.py
cat agency_os/core_system/runtime/providers/google.py

# Check integration
grep -n "provider" agency_os/core_system/runtime/llm_client.py | head -20

# Test provider system
uv run pytest tests/test_providers.py -v
```

---

## Provider Implementations

### AnthropicProvider (Claude)

**Supported Models:**
- `claude-3-5-sonnet-20241022` (default)
- `claude-3-5-sonnet-20240620`
- `claude-3-opus-20240229`
- `claude-3-haiku-20240307`

**Configuration:**
```bash
ANTHROPIC_API_KEY=sk-ant-...
VIBE_MODEL_PROVIDER=anthropic
VIBE_MODEL_MODEL_NAME=claude-3-5-sonnet-20241022
```

**Pricing (Claude 3.5 Sonnet):**
```
Input:  $3.00 per million tokens
Output: $15.00 per million tokens
```

### GoogleProvider (Gemini)

**Supported Models:**
- `gemini-2.5-flash-exp` (default, experimental)
- `gemini-2.0-flash-exp`
- `gemini-1.5-pro`
- `gemini-1.5-flash`

**Configuration:**
```bash
GOOGLE_API_KEY=AIza...
VIBE_MODEL_PROVIDER=google
VIBE_MODEL_MODEL_NAME=gemini-2.5-flash-exp
```

**Pricing (Gemini 2.5 Flash - Experimental):**
```
Input:  $0.00 (free tier during preview)
Output: $0.00 (free tier during preview)
```

### NoOpProvider (Fallback)

**Purpose:** Graceful degradation when no real provider available

**Behavior:**
```python
response = NoOpProvider().invoke(prompt="Test")

# Returns:
{
    "content": "{}",              # Empty JSON
    "usage": {
        "input_tokens": 0,
        "output_tokens": 0,
        "cost_usd": 0.0
    },
    "model": "noop",
    "finish_reason": "no_provider",
    "provider": "noop"
}
```

---

## Usage Examples

### Automatic Provider Selection

```python
from agency_os.core_system.runtime.llm_client import LLMClient

# Auto-detects provider based on available API keys
client = LLMClient()

# Invoke with auto-selected provider
response = client.invoke(
    prompt="What is 2+2?",
    model="claude-3-5-sonnet-20241022",  # Provider-specific model
    max_tokens=100
)

print(f"Provider: {client.provider.get_provider_name()}")
print(f"Response: {response.content}")
print(f"Cost: ${response.usage.cost_usd:.4f}")
```

### Explicit Provider Selection

```python
from agency_os.core_system.runtime.providers import create_provider, AnthropicProvider
from agency_os.core_system.runtime.llm_client import LLMClient

# Create specific provider
anthropic = AnthropicProvider(api_key="sk-ant-...")

# Use with LLM client
client = LLMClient(provider=anthropic)

response = client.invoke(
    prompt="Explain quantum computing",
    model="claude-3-5-sonnet-20241022",
    max_tokens=1024
)
```

### Provider Failover

```python
from agency_os.core_system.runtime.providers import create_provider

# Try Google first, fallback to Anthropic
try:
    provider = create_provider("google")
    if not provider.is_available():
        provider = create_provider("anthropic")
except Exception:
    provider = create_provider("noop")  # Final fallback

client = LLMClient(provider=provider)
```

---

## Configuration

### Environment Variables

**Provider Selection:**
```bash
VIBE_MODEL_PROVIDER=anthropic|google|openai|local
VIBE_MODEL_MODEL_NAME=model-identifier
```

**API Keys:**
```bash
ANTHROPIC_API_KEY=sk-ant-...
GOOGLE_API_KEY=AIza...
OPENAI_API_KEY=sk-...
```

**Model Parameters:**
```bash
VIBE_MODEL_MAX_TOKENS=4096
VIBE_MODEL_TEMPERATURE=1.0
```

### Phoenix Config Integration

```python
from agency_os.config import get_config

config = get_config()

print(f"Provider: {config.model.provider}")
print(f"Model: {config.model.model_name}")
print(f"API Key: {'âœ… Set' if config.model.api_key else 'âŒ Not set'}")
```

---

## Provider Interface Contract

All providers must implement:

```python
class LLMProvider(ABC):
    @abstractmethod
    def invoke(
        self,
        prompt: str,
        model: str,
        max_tokens: int = 4096,
        temperature: float = 1.0,
        **kwargs
    ) -> LLMResponse:
        """Invoke LLM with prompt, return standardized response"""
        pass

    @abstractmethod
    def calculate_cost(
        self,
        input_tokens: int,
        output_tokens: int,
        model: str
    ) -> float:
        """Calculate cost for token usage"""
        pass

    @abstractmethod
    def get_available_models(self) -> list[str]:
        """Return list of supported models"""
        pass

    @abstractmethod
    def is_available(self) -> bool:
        """Check if provider is available (API key set, etc.)"""
        pass
```

**Standardized Response:**
```python
@dataclass
class LLMResponse:
    content: str              # LLM response text
    usage: LLMUsage          # Token usage and cost
    model: str               # Model used
    finish_reason: str       # Completion reason
    provider: str            # Provider name (anthropic, google, etc.)
```

---

## Cross-Pillar Dependencies

**GAD-511 depends on:**
- **GAD-100 (Phoenix Config):** Model configuration loading
- **GAD-509 (CircuitBreaker):** Wraps provider calls for resilience
- **GAD-510 (QuotaManager):** Enforces cost limits on provider calls

**Other pillars depend on GAD-511:**
- **GAD-2XX (Orchestration):** Uses LLMClient for agent execution
- **GAD-3XX (Agent Framework):** Agents invoke LLMs through provider system

---

## Known Issues & Limitations

### Current Limitations

1. **OpenAI Not Implemented:** Factory recognizes but returns NoOp
2. **Local/Ollama Not Implemented:** Factory recognizes but returns NoOp
3. **Single Provider Per Session:** No runtime provider switching
4. **Static Pricing:** Pricing hardcoded in provider implementations

### Future Enhancements (Deferred)

**GAD-511.2: OpenAI Provider**
- Full GPT-4 / GPT-3.5 support
- OpenAI-specific features (functions, tools)

**GAD-511.3: Local Provider (Ollama)**
- Local model execution
- Privacy-preserving deployment
- Offline operation

**GAD-511.4: Multi-Provider Routing**
- Smart routing based on cost/latency/capabilities
- Automatic failover on provider outage
- Load balancing across providers

**GAD-511.5: Dynamic Pricing**
- Load pricing from provider APIs
- Real-time cost optimization
- Budget-aware provider selection

---

## Testing Strategy

### Unit Tests (Target)

```python
def test_provider_auto_detection():
    """Factory should detect provider from API keys"""
    os.environ["GOOGLE_API_KEY"] = "test-key"
    provider = get_default_provider()
    assert isinstance(provider, GoogleProvider)

def test_noop_fallback():
    """Factory should return NoOp when no keys available"""
    # Clear all API keys
    for key in ["GOOGLE_API_KEY", "ANTHROPIC_API_KEY", "OPENAI_API_KEY"]:
        os.environ.pop(key, None)

    provider = get_default_provider()
    assert isinstance(provider, NoOpProvider)

def test_standardized_response_format():
    """All providers should return standardized response"""
    provider = AnthropicProvider(api_key="test")
    response = provider.invoke(prompt="test", model="claude-3-5-sonnet")

    assert hasattr(response, "content")
    assert hasattr(response, "usage")
    assert hasattr(response, "model")
    assert hasattr(response, "provider")
```

---

## Migration Notes

**Created:** 2025-11-18
**First Implementation:** PR #156, #157, #158

**Key Commits:**
- `fdabc18`: feat(GAD-511): Implement Neural Adapter Strategy - Multi-Provider LLM Support
- `8cf9cf0`: feat(GAD-511): Add Google Gemini provider support
- `4987afd`: feat(GAD-511): Upgrade to Gemini 2.5 Flash (experimental)
- `eeca679`: feat(GAD-511): Implement Golden Thread verification and fix runtime imports

**Related Documentation:**
- This document created as part of Operation Glass Box (documentation sync)

---

## References

**Design Patterns:**
- Gang of Four: [Strategy Pattern](https://refactoring.guru/design-patterns/strategy)
- Martin Fowler: [Strategy Pattern](https://martinfowler.com/eaaCatalog/strategy.html)

**Provider Documentation:**
- [Anthropic Claude API](https://docs.anthropic.com/claude/reference)
- [Google Gemini API](https://ai.google.dev/docs)
- [OpenAI API](https://platform.openai.com/docs/api-reference)

**Related GADs:**
- **GAD-509:** Circuit Breaker Protocol (resilience layer)
- **GAD-510:** Quota Manager (cost control layer)
- **GAD-500:** Runtime Engineering (self-regulating execution)
- **GAD-100:** Phoenix Configuration System

---

**Last Updated:** 2025-11-19
**Next Review:** After OpenAI provider implementation
**Maintainer:** Runtime Engineering Team
