#!/usr/bin/env python3
"""
VIBE-CLI - Agency OS Command Line Interface
============================================

This is the "Flie√üband" (conveyor belt) that connects Claude Code (the "Brain")
to the orchestrator (the "Arm").

Architecture (MVP - DELEGATION ONLY):
- Launches core_orchestrator.py in delegated mode
- Monitors STDOUT for INTELLIGENCE_REQUESTs
- Delegates prompts to Claude Code operator (via STDOUT/STDIN)
- Receives responses from Claude Code and forwards to orchestrator

Version: 1.0 (Delegation Mode - No Direct API Calls)
"""

# ============================================================================
# LAYER 2: UV ENVIRONMENT CHECK (Fail Fast with Guidance)
# ============================================================================
import sys
from pathlib import Path


def _ensure_uv_environment():
    """
    Ensure UV environment is set up (Layer 2 of 4-layer defense).

    Post-UV migration strategy:
    - Instead of auto-installing with pip (conflicts with uv.lock),
      check if .venv exists and guide users to proper setup.
    - This prevents pip/uv mixing and maintains deterministic builds.
    - Users who follow Layer 1 (devcontainer) or Layer 3 (setup.sh)
      never hit this check.

    Layer 2 catches users who:
    - Skip setup entirely
    - Run vibe-cli directly without proper environment

    Why not auto-install?
    - pip install conflicts with uv.lock (non-deterministic)
    - uv sync requires uv command (may not be available)
    - Better to guide users to canonical setup: make install

    Related: DEFENSE_VALIDATION_REPORT.md, MIGRATION_NOTES.md
    """
    repo_root = Path(__file__).parent
    venv_path = repo_root / ".venv"

    # Check if UV environment exists
    if not venv_path.exists():
        print("‚ùå Error: UV environment not found", file=sys.stderr)
        print("", file=sys.stderr)
        print("It looks like dependencies aren't installed yet.", file=sys.stderr)
        print("", file=sys.stderr)
        print("Please run ONE of these commands:", file=sys.stderr)
        print("  make install          # Recommended (uses Makefile)", file=sys.stderr)
        print("  ./setup.sh            # Alternative (uses setup script)", file=sys.stderr)
        print("  uv sync --all-extras  # Direct (if uv is installed)", file=sys.stderr)
        print("", file=sys.stderr)
        print("This will create a .venv/ directory with all dependencies", file=sys.stderr)
        print("from the locked versions in uv.lock (deterministic builds).", file=sys.stderr)
        print("", file=sys.stderr)
        print("Why this error?", file=sys.stderr)
        print(
            "  vibe-agency uses UV for dependency management (10-15x faster than pip).",
            file=sys.stderr,
        )
        print("  Dependencies are locked in uv.lock for reproducibility.", file=sys.stderr)
        print("  See: MIGRATION_NOTES.md for more info.", file=sys.stderr)
        sys.exit(1)

    # Check if we're running in the UV environment
    # Note: Not strictly necessary (imports will fail if wrong env)
    # but provides better error message
    if not sys.prefix == str(venv_path):
        # Running outside .venv - might be manual activation or system Python
        # Don't fail hard, but warn (imports will fail if truly wrong)
        import os

        if not os.getenv("VIRTUAL_ENV"):  # Not in any venv
            print("‚ö†Ô∏è  Warning: Not running in UV environment", file=sys.stderr)
            print("", file=sys.stderr)
            print("Tip: Use 'uv run' to ensure correct environment:", file=sys.stderr)
            print("  uv run ./vibe-cli run <project-id>", file=sys.stderr)
            print("", file=sys.stderr)
            # Don't exit - let imports fail naturally if deps are wrong


# Execute environment check BEFORE any other imports
_ensure_uv_environment()

import json  # noqa: E402
import logging  # noqa: E402

# ============================================================================
# IMPORTS (After Dependency Check)
# ============================================================================
import subprocess  # noqa: E402
from typing import Any, Dict, List, Optional  # noqa: E402

import yaml  # noqa: E402

logging.basicConfig(level=logging.INFO, format="[%(levelname)s] %(message)s")
logger = logging.getLogger(__name__)


# ============================================================================
# GAD-005: RUNTIME ENGINEERING - UNAVOIDABLE MOTD
# Helper functions for displaying Message of the Day before execution
# ============================================================================


def load_system_status() -> Dict[str, Any]:
    """
    Load .system_status.json from repository root.

    Returns:
        System status dict (empty if file missing)
    """
    status_file = Path(".system_status.json")
    if status_file.exists():
        try:
            with open(status_file) as f:
                return json.load(f)
        except Exception as e:
            logger.warning(f"Failed to load system status: {e}")
            return {}
    return {}


def load_session_handoff() -> Optional[Dict[str, Any]]:
    """
    Load .session_handoff.json if exists.

    Returns:
        Session handoff dict or None if missing
    """
    handoff_file = Path(".session_handoff.json")
    if handoff_file.exists():
        try:
            with open(handoff_file) as f:
                return json.load(f)
        except Exception as e:
            logger.warning(f"Failed to load session handoff: {e}")
            return None
    return None


def is_status_stale(timestamp_str: str, hours: int = 1) -> bool:
    """
    Check if system status timestamp is stale (older than N hours).

    Args:
        timestamp_str: ISO format timestamp
        hours: Staleness threshold (default: 1 hour)

    Returns:
        True if stale, False if fresh

    BURN THE GHEE: Replaces redundant subprocess call to update-system-status.sh
    """
    from datetime import datetime, timedelta, timezone

    try:
        # Parse ISO timestamp
        ts = datetime.fromisoformat(timestamp_str.replace("Z", "+00:00"))
        now = datetime.now(timezone.utc)
        age = now - ts
        return age > timedelta(hours=hours)
    except Exception:
        # If parsing fails, assume stale
        return True


def format_git_status(status: Dict) -> str:
    """
    Format git status for MOTD display.

    Args:
        status: System status dict

    Returns:
        Formatted git status string
    """
    git = status.get("git", {})
    if git.get("working_directory_clean"):
        branch = git.get("branch", "unknown")
        return f"‚úÖ Clean (branch: {branch})"
    else:
        return "‚ö†Ô∏è  Uncommitted changes"


def format_linting_status(status: Dict) -> str:
    """
    Format linting status for MOTD display.

    Args:
        status: System status dict

    Returns:
        Formatted linting status string
    """
    linting = status.get("linting", {})
    status_val = linting.get("status", "unknown")

    if status_val == "passing":
        return "‚úÖ Passing (0 errors)"
    elif status_val == "failing":
        errors = linting.get("errors_count", 0)
        return f"‚ùå Failing ({errors} errors)"
    else:
        return f"‚ö†Ô∏è  {status_val}"


def format_test_status(status: Dict) -> str:
    """
    Format test status for MOTD display.

    Args:
        status: System status dict

    Returns:
        Formatted test status string
    """
    tests = status.get("tests", {})
    planning = tests.get("planning_workflow", "unknown")

    if planning == "passing":
        return "‚úÖ Passing"
    elif planning == "failing":
        return "‚ùå Failing"
    else:
        return f"‚ö†Ô∏è  {planning}"


def format_integrity_status() -> str:
    """Format system integrity status for MOTD display"""
    script_path = Path("scripts/verify-system-integrity.py")

    if not script_path.exists():
        return "‚ö†Ô∏è  Script not found"

    try:
        result = subprocess.run(
            ["python", str(script_path)],
            capture_output=True,
            text=True,
            timeout=5,
            check=False
        )

        if result.returncode == 0:
            return "‚úÖ Verified"
        else:
            return "‚ùå Compromised"

    except Exception:
        return "‚ö†Ô∏è  Check failed"


def verify_system_integrity() -> bool:
    """
    Verify system integrity before any operation (Layer 0).

    Runs scripts/verify-system-integrity.py to check that all critical
    regulatory files match their trusted checksums.

    Returns:
        bool: True if verification passed, False otherwise

    GAD-005-ADDITION: Layer 0 - System Integrity Verification
    """
    import subprocess

    script_path = Path("scripts/verify-system-integrity.py")

    # Check if verification script exists
    if not script_path.exists():
        print("‚ö†Ô∏è  Warning: System integrity script not found", file=sys.stderr)
        print(f"    Expected: {script_path}", file=sys.stderr)
        print("    Skipping integrity check...", file=sys.stderr)
        print()
        return True  # Non-fatal if script doesn't exist yet

    # Run verification
    try:
        result = subprocess.run(
            ["python", str(script_path)],
            capture_output=True,
            text=True,
            timeout=5,
            check=False
        )

        if result.returncode == 0:
            return True
        else:
            # Verification failed - print output and return False
            print(result.stdout, file=sys.stderr)
            return False

    except subprocess.TimeoutExpired:
        print("‚ö†Ô∏è  Warning: System integrity check timed out", file=sys.stderr)
        print("    Continuing with caution...", file=sys.stderr)
        print()
        return True  # Non-fatal timeout

    except Exception as e:
        print(f"‚ö†Ô∏è  Warning: System integrity check failed: {e}", file=sys.stderr)
        print("    Continuing with caution...", file=sys.stderr)
        print()
        return True  # Non-fatal exception


def get_critical_alerts(status: Dict[str, Any]) -> List[str]:
    """
    Extract top 3 critical alerts that agent MUST see (GAD-502 Phase 4).

    Priority:
    1. System integrity failures (CRITICAL - system compromised)
    2. Linting errors (blocks commits)
    3. Test failures (indicates broken code)
    4. Dirty git (warns before transitions)

    Args:
        status: System status dict from .system_status.json

    Returns:
        List of alert strings (max 3 items)
    """
    alerts = []

    # System integrity (highest priority)
    if not status.get("system_integrity", {}).get("verified", True):
        alerts.append("‚ùå SYSTEM INTEGRITY FAILED - Run: python scripts/verify-system-integrity.py")

    # Linting (blocks commits)
    linting = status.get("linting", {})
    if linting.get("status") == "failing":
        count = linting.get("error_count", 0)
        alerts.append(f"‚ùå LINTING FAILED ({count} errors) - Run: uv run ruff check . --fix")

    # Tests (broken code)
    tests = status.get("tests", {})
    if tests.get("status") == "failing":
        count = tests.get("failed", 0)
        alerts.append(f"‚ùå TESTS FAILING ({count} failed) - Run: uv run pytest")

    # Git (warns before operations)
    git = status.get("git", {})
    if git.get("status") == "dirty":
        count = len(git.get("uncommitted_files", []))
        alerts.append(f"‚ö†Ô∏è  DIRTY GIT ({count} files uncommitted)")

    return alerts[:3]  # Max 3 alerts


def display_motd():
    """
    Display Message of the Day before any vibe-cli execution.

    This makes critical context UNAVOIDABLE - agents see it in stdout.

    GAD-005: Runtime Engineering - Unavoidable MOTD (Component A)
    GAD-502 Phase 4: Critical alerts shown first (Haiku-readable)
    BURN THE GHEE Phase 1: Optimized 4-layer handoff + removed redundant status update
    """
    # Load data (trust git hooks to keep status fresh)
    status = load_system_status()
    handoff = load_session_handoff()

    # Check if status is stale (warn but don't block)
    timestamp = status.get("timestamp", "")
    if timestamp and is_status_stale(timestamp, hours=1):
        logger.warning("‚ö†Ô∏è  System status is >1 hour old. Run: ./bin/update-system-status.sh")

    # Render MOTD
    print("‚ïê" * 60)
    print("üöÄ VIBE AGENCY - RUNTIME ENGINEERING SESSION")
    print("‚ïê" * 60)
    print()

    # CRITICAL ALERTS (GAD-502 Phase 4: Haiku Hardening)
    critical_alerts = get_critical_alerts(status)
    if critical_alerts:
        print("üö® CRITICAL ALERTS (READ THIS FIRST!)")
        print("‚îÄ" * 60)
        for alert in critical_alerts:
            print(f"  {alert}")
        print()
    else:
        print("‚úÖ No critical alerts - system healthy")
        print()

    # System health
    print("üìä SYSTEM HEALTH")
    print(f"  Git: {format_git_status(status)}")
    print(f"  Linting: {format_linting_status(status)}")
    print(f"  Tests: {format_test_status(status)}")
    print(f"  System Integrity: {format_integrity_status()}")

    # Steward principles (bedrock operating principles)
    steward = status.get("steward", "")
    if steward:
        print(f"  Steward: {steward}")
    print()

    # Session handoff (4-layer optimized structure)
    if handoff:
        # Layer 0: Bedrock (always show)
        layer0 = handoff.get("layer0_bedrock", {})
        layer1 = handoff.get("layer1_runtime", {})

        # Fallback to old structure if new layers don't exist
        if not layer0:
            # Old format compatibility
            layer0 = {
                "from": handoff.get("from_agent", "Unknown"),
                "date": handoff.get("date", "Unknown"),
                "state": "legacy_format",
                "blocker": None,
            }
            layer1 = {
                "completed_summary": "Legacy handoff format (pre-optimization)",
                "todos": handoff.get("recommendations_for_next_session", [])[:3],
                "critical_files": [],
            }

        print("üìã SESSION HANDOFF")
        print(f"  From: {layer0.get('from', 'Unknown')}")
        print(f"  Date: {layer0.get('date', 'Unknown')}")
        print(f"  State: {layer0.get('state', 'Unknown')}")

        # Show blocker if exists (instant visibility!)
        if layer0.get("blocker"):
            print(f"  ‚ö†Ô∏è  Blocker: {layer0['blocker']}")

        print()

        # Layer 1: Runtime context
        summary = layer1.get("completed_summary", "")
        if summary:
            print(f"  Summary: {summary}")
            print()

        # TODOs from Layer 1
        todos = layer1.get("todos", [])
        if todos:
            print("  Your TODOs:")
            for todo in todos[:3]:
                print(f"    ‚Üí {todo}")
            if len(todos) > 3:
                print(f"    ... and {len(todos) - 3} more")
            print()

        # Critical files from Layer 1
        files = layer1.get("critical_files", [])
        if files:
            print("  Critical files:")
            for file in files[:3]:
                print(f"    üìÑ {file}")
            if len(files) > 3:
                print(f"    ... and {len(files) - 3} more")
            print()

    # Quick commands
    print("üí° QUICK COMMANDS")
    print("  ./bin/show-context.py    - Full system context")
    print("  ./bin/health-check.sh    - System health check")
    print("  ./bin/pre-push-check.sh  - Pre-push quality checks")
    print()
    print("‚ïê" * 60)
    print()


class VibeCLI:
    """
    Main CLI class that manages the orchestrator <-> Claude Code handoff.
    """

    def __init__(self, repo_root: Path):
        """
        Initialize vibe-cli.

        Args:
            repo_root: Root of vibe-agency repo

        Note: MVP uses DELEGATION ONLY mode.
              vibe-cli acts as a bridge between Claude Code (operator) and orchestrator.
              No direct API calls - intelligence is delegated to Claude Code operator.
        """
        self.repo_root = Path(repo_root)

        # Orchestrator path
        self.orchestrator_path = (
            self.repo_root / "agency_os" / "00_system" / "orchestrator" / "core_orchestrator.py"
        )

        if not self.orchestrator_path.exists():
            raise FileNotFoundError(f"Orchestrator not found: {self.orchestrator_path}")

        # Tool definitions path (GAD-003)
        self.tool_definitions_path = (
            self.repo_root
            / "agency_os"
            / "00_system"
            / "orchestrator"
            / "tools"
            / "tool_definitions.yaml"
        )

        # Tool executor path (for local tool execution)
        self.tool_executor_path = (
            self.repo_root / "agency_os" / "00_system" / "orchestrator" / "tools"
        )

        # Playbook registry path
        self.playbook_registry_path = (
            self.repo_root / "docs" / "playbook" / "_registry.yaml"
        )

    def run_project(self, project_id: str, mode: str = "delegated", resume: bool = False) -> None:
        """
        Run SDLC for a project with Claude Code integration.

        Args:
            project_id: Project ID
            mode: Execution mode (delegated or autonomous)
            resume: Resume from last handoff checkpoint
        """
        logger.info(f"üöÄ Starting Agency OS for project: {project_id}")
        logger.info(f"   Execution mode: {mode}")
        logger.info(f"   Repository: {self.repo_root}")

        if resume:
            logger.info("   üîÑ Resume mode: Loading last handoff checkpoint")

        if mode == "delegated":
            self._run_delegated(project_id, resume=resume)
        elif mode == "autonomous":
            self._run_autonomous(project_id, resume=resume)
        else:
            raise ValueError(f"Invalid mode: {mode}")

    def _run_delegated(self, project_id: str, resume: bool = False) -> None:
        """
        Run orchestrator in delegated mode with STDIN/STDOUT handoff.

        This method:
        1. Launches orchestrator as subprocess
        2. Monitors STDOUT for INTELLIGENCE_REQUESTs
        3. Delegates prompts to Claude Code operator
        4. Sends responses back via STDIN

        Args:
            project_id: Project ID
            resume: Resume from last handoff checkpoint
        """
        logger.info("üîó Launching orchestrator in delegated mode...")

        # Launch orchestrator
        cmd = [
            sys.executable,
            str(self.orchestrator_path),
            str(self.repo_root),
            project_id,
            "--mode=delegated",
        ]

        if resume:
            cmd.append("--resume")

        logger.info(f"   Command: {' '.join(cmd)}")

        # Start process with pipes
        process = subprocess.Popen(
            cmd,
            stdin=subprocess.PIPE,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            bufsize=1,  # Line buffered
        )

        logger.info("‚úÖ Orchestrator launched (PID: {})".format(process.pid))

        # Monitor process output
        self._monitor_orchestrator(process)

    def _monitor_orchestrator(self, process: subprocess.Popen) -> None:
        """
        Monitor orchestrator STDOUT/STDERR and handle intelligence requests.

        Args:
            process: Orchestrator subprocess
        """
        intelligence_request_buffer = []
        capturing_request = False

        try:
            while True:
                # Read from STDOUT
                line = process.stdout.readline()

                if not line:
                    # Process finished
                    break

                # Check for intelligence request markers
                if "---INTELLIGENCE_REQUEST_START---" in line:
                    capturing_request = True
                    intelligence_request_buffer = []
                    logger.info("üì© Intelligence request detected...")
                    continue

                if "---INTELLIGENCE_REQUEST_END---" in line:
                    capturing_request = False
                    # Parse and handle request
                    request_json = "".join(intelligence_request_buffer)
                    self._handle_intelligence_request(request_json, process)
                    continue

                if capturing_request:
                    intelligence_request_buffer.append(line)
                else:
                    # Regular output - print to console
                    print(line, end="")

                # Also check stderr for log messages
                # (non-blocking read would be better, but this works for now)

        except KeyboardInterrupt:
            logger.info("\n‚è∏Ô∏è  Interrupted by user")
            process.terminate()

        except Exception as e:
            logger.error(f"‚ùå Error monitoring orchestrator: {e}")
            process.terminate()
            raise

        finally:
            # Wait for process to finish
            return_code = process.wait()
            if return_code == 0:
                logger.info("‚úÖ Orchestrator finished successfully")
                # GAD-100 Phase 3: Auto-create session handoff
                self._auto_create_handoff()
            else:
                logger.error(f"‚ùå Orchestrator failed with code {return_code}")

    def _handle_intelligence_request(self, request_json: str, process: subprocess.Popen) -> None:
        """
        Handle an intelligence request from orchestrator.

        Args:
            request_json: JSON string of intelligence request
            process: Orchestrator subprocess
        """
        try:
            # Parse request
            request = json.loads(request_json)

            agent = request.get("agent")
            task_id = request.get("task_id")
            prompt = request.get("prompt")
            context = request.get("context", {})

            logger.info("ü§ñ Intelligence Request:")
            logger.info(f"   Agent: {agent}")
            logger.info(f"   Task: {task_id}")
            logger.info(f"   Context: {context}")
            logger.info(f"   Prompt length: {len(prompt)} chars")

            # Get project_id from context
            project_id = context.get("project_id", "unknown-project")

            # Delegate to Claude Code operator (FILE-BASED DELEGATION)
            logger.info("‚öôÔ∏è  Delegating prompt to Claude Code operator...")
            result = self._delegate_to_operator(prompt, agent, task_id, project_id)

            # Build response
            response = {
                "type": "INTELLIGENCE_RESPONSE",
                "agent": agent,
                "task_id": task_id,
                "result": result,
                "metadata": {"executor": "vibe-cli", "model": "claude-3-5-sonnet-20241022"},
            }

            # Send response to orchestrator via STDIN
            response_json = json.dumps(response)
            logger.info("üì§ Sending intelligence response...")
            process.stdin.write(response_json + "\n")
            process.stdin.flush()
            logger.info("‚úÖ Intelligence response sent")

        except Exception as e:
            logger.error(f"‚ùå Failed to handle intelligence request: {e}")
            # Send error response
            error_response = {
                "type": "INTELLIGENCE_RESPONSE",
                "result": {"error": str(e)},
                "metadata": {"error": True},
            }
            process.stdin.write(json.dumps(error_response) + "\n")
            process.stdin.flush()

    def _load_tools_for_agent(self, agent: str) -> List[Dict[str, Any]]:
        """
        Load tool definitions for a specific agent.

        Args:
            agent: Agent name (e.g., "MARKET_RESEARCHER")

        Returns:
            List of Anthropic tool schemas
        """
        # Map agent names to their composition files
        agent_paths = {
            "MARKET_RESEARCHER": "agency_os/01_planning_framework/agents/research/MARKET_RESEARCHER/_composition.yaml",
            "TECH_RESEARCHER": "agency_os/01_planning_framework/agents/research/TECH_RESEARCHER/_composition.yaml",
            "FACT_VALIDATOR": "agency_os/01_planning_framework/agents/research/FACT_VALIDATOR/_composition.yaml",
            "USER_RESEARCHER": "agency_os/01_planning_framework/agents/research/USER_RESEARCHER/_composition.yaml",
        }

        # Check if agent has tools
        if agent not in agent_paths:
            logger.debug(f"Agent {agent} not in tool-enabled agents list, returning empty tools")
            return []

        # Load agent composition
        composition_path = self.repo_root / agent_paths[agent]
        if not composition_path.exists():
            logger.warning(f"Composition file not found: {composition_path}")
            return []

        with open(composition_path, "r") as f:
            composition = yaml.safe_load(f)

        # Extract tool names
        tool_names = composition.get("tools", [])
        if not tool_names:
            logger.debug(f"Agent {agent} has no tools configured")
            return []

        # Load tool definitions
        if not self.tool_definitions_path.exists():
            logger.warning(f"Tool definitions not found: {self.tool_definitions_path}")
            return []

        with open(self.tool_definitions_path, "r") as f:
            tool_definitions = yaml.safe_load(f)

        # Convert to Anthropic schema
        tools = []
        for tool_name in tool_names:
            if tool_name in tool_definitions.get("tools", {}):
                tool_def = tool_definitions["tools"][tool_name]
                anthropic_tool = self._convert_yaml_to_anthropic_schema(tool_name, tool_def)
                tools.append(anthropic_tool)
            else:
                logger.warning(f"Tool {tool_name} not found in tool_definitions.yaml")

        logger.info(f"Loaded {len(tools)} tools for agent {agent}: {[t['name'] for t in tools]}")
        return tools

    def _convert_yaml_to_anthropic_schema(
        self, tool_name: str, tool_def: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Convert YAML tool definition to Anthropic's JSON tool schema.

        Args:
            tool_name: Tool name (e.g., "google_search")
            tool_def: YAML tool definition

        Returns:
            Anthropic tool schema (JSON)
        """
        # Build input schema
        properties = {}
        required = []

        for param_name, param_def in tool_def.get("parameters", {}).items():
            properties[param_name] = {
                "type": param_def.get("type", "string"),
                "description": param_def.get("description", ""),
            }

            # Add default if specified
            if "default" in param_def:
                properties[param_name]["default"] = param_def["default"]

            # Track required parameters
            if param_def.get("required", False):
                required.append(param_name)

        # Return Anthropic tool schema
        return {
            "name": tool_name,
            "description": tool_def.get("description", ""),
            "input_schema": {"type": "object", "properties": properties, "required": required},
        }

    def _execute_tools(self, tool_use_blocks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Execute tool calls locally using tool_executor.py.

        Args:
            tool_use_blocks: List of tool_use content blocks from Anthropic API

        Returns:
            List of tool results
        """
        # Import tool executor (lazy import to avoid circular dependencies)
        sys.path.insert(0, str(self.tool_executor_path))
        try:
            from tool_executor import ToolExecutor
        except ImportError:
            logger.error("Failed to import tool_executor - tool execution disabled")
            return [{"error": "Tool executor not available"}]

        # Initialize tool executor
        executor = ToolExecutor()

        # Execute each tool
        results = []
        for tool_use in tool_use_blocks:
            if tool_use.get("type") != "tool_use":
                continue

            tool_name = tool_use.get("name")
            tool_input = tool_use.get("input", {})
            tool_use_id = tool_use.get("id")

            logger.info(f"Executing tool: {tool_name} with input: {tool_input}")

            try:
                # Execute tool
                result = executor.execute_tool(tool_name, tool_input)
                results.append(
                    {
                        "type": "tool_result",
                        "tool_use_id": tool_use_id,
                        "content": json.dumps(result),
                    }
                )
                logger.info(f"Tool {tool_name} executed successfully")

            except Exception as e:
                logger.error(f"Tool {tool_name} execution failed: {e}")
                results.append(
                    {
                        "type": "tool_result",
                        "tool_use_id": tool_use_id,
                        "content": json.dumps({"error": str(e)}),
                        "is_error": True,
                    }
                )

        return results

    def _delegate_to_operator(
        self, prompt: str, agent: str, task_id: str, project_id: str
    ) -> Dict[str, Any]:
        """
        Delegate prompt execution to Claude Code operator via FILE-BASED exchange.

        Changed from STDIN/STDOUT to file-based for Browser environment compatibility.

        Architecture:
        1. Write delegation request to .delegation/request_{uuid}.json
        2. Poll for .delegation/response_{uuid}.json
        3. Read response, cleanup files, return result

        Args:
            prompt: Prompt text
            agent: Agent name
            task_id: Task ID
            project_id: Project ID (for workspace path)

        Returns:
            Parsed response from Claude Code operator

        Note: File-based delegation (Nov 16 fix).
              See: docs/architecture/EXECUTION_MODE_STRATEGY.md
              See: ARCHITECTURE_BREAKDOWN_REPORT.md
        """
        import time
        import uuid
        from datetime import datetime

        # Create delegation directory
        delegation_dir = self.repo_root / "workspaces" / project_id / ".delegation"
        delegation_dir.mkdir(parents=True, exist_ok=True)

        # Generate unique request ID
        request_id = str(uuid.uuid4())
        request_file = delegation_dir / f"request_{request_id}.json"
        response_file = delegation_dir / f"response_{request_id}.json"

        # Build delegation request
        delegation_request = {
            "type": "INTELLIGENCE_DELEGATION",
            "request_id": request_id,
            "agent": agent,
            "task_id": task_id,
            "prompt": prompt,
            "timestamp": datetime.now().isoformat(),
            "metadata": {
                "delegator": "vibe-cli",
                "mode": "file_based_delegation",
                "request_file": str(request_file),
                "response_file": str(response_file),
            },
        }

        # Write request file
        with open(request_file, "w") as f:
            json.dump(delegation_request, f, indent=2)

        logger.info(f"üì§ Delegation request written: {request_file}")
        logger.info("‚è≥ Waiting for response from Claude Code operator...")
        logger.info(f"   Expected response file: {response_file}")

        # Poll for response file
        timeout = 600  # 10 minutes
        start_time = time.time()
        poll_interval = 0.5  # Check every 500ms

        while not response_file.exists():
            elapsed = time.time() - start_time
            if elapsed > timeout:
                # Cleanup request file on timeout
                if request_file.exists():
                    request_file.unlink()
                raise TimeoutError(f"No response after {timeout}s. Expected file: {response_file}")

            time.sleep(poll_interval)

            # Log progress every 10 seconds
            if int(elapsed) % 10 == 0 and int(elapsed) > 0:
                logger.info(f"   Still waiting... ({int(elapsed)}s elapsed)")

        # Read response
        try:
            with open(response_file) as f:
                response = json.load(f)

            logger.info("‚úÖ Response received from Claude Code operator")

            # Cleanup files
            if request_file.exists():
                request_file.unlink()
            if response_file.exists():
                response_file.unlink()

            return response.get("result", {})

        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse response file: {e}")
            # Cleanup on error
            if request_file.exists():
                request_file.unlink()
            if response_file.exists():
                response_file.unlink()
            raise RuntimeError(f"Invalid response JSON in {response_file}: {e}")
        except Exception as e:
            logger.error(f"Error reading response: {e}")
            # Cleanup on error
            if request_file.exists():
                request_file.unlink()
            if response_file.exists():
                response_file.unlink()
            raise

    def _auto_create_handoff(self) -> None:
        """
        Automatically create session handoff after orchestrator finishes.

        Related: GAD-100 Phase 3 - Handoff Automation
        """
        import subprocess

        script_path = self.repo_root / "bin" / "create-session-handoff.sh"

        if script_path.exists():
            try:
                subprocess.run(
                    [str(script_path), "--auto"],
                    cwd=self.repo_root,
                    check=True
                )
                logger.info("‚úÖ Session handoff auto-created")
            except subprocess.CalledProcessError as e:
                logger.warning(f"‚ö†Ô∏è  Failed to auto-create handoff: {e}")
        else:
            logger.warning("‚ö†Ô∏è  Handoff script not found, skipping auto-creation")

    def _run_autonomous(self, project_id: str, resume: bool = False) -> None:
        """
        Run orchestrator in autonomous mode (direct execution, no handoff).

        Args:
            project_id: Project ID
        """
        logger.info("‚ö° Running in autonomous mode (no Claude Code integration)")

        # Launch orchestrator directly
        cmd = [
            sys.executable,
            str(self.orchestrator_path),
            str(self.repo_root),
            project_id,
            "--mode=autonomous",
        ]

        result = subprocess.run(cmd)
        sys.exit(result.returncode)

    def _load_playbook_registry(self) -> Optional[Dict[str, Any]]:
        """
        Load playbook registry from docs/playbook/_registry.yaml.

        Returns:
            Registry dict or None if file doesn't exist
        """
        if not self.playbook_registry_path.exists():
            logger.warning(f"Playbook registry not found: {self.playbook_registry_path}")
            return None

        try:
            with open(self.playbook_registry_path) as f:
                return yaml.safe_load(f)
        except Exception as e:
            logger.error(f"Failed to load playbook registry: {e}")
            return None

    def _match_playbook(self, user_input: str, registry: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        Match user input against playbook registry using Tier 1 keyword matching.

        Strategy:
        - Tier 1: Simple keyword matching (fast, no dependencies)
        - Future: Tier 2 can add semantic search with embeddings

        Args:
            user_input: User's intent
            registry: Loaded playbook registry

        Returns:
            Matched route dict or None
        """
        routes = registry.get("routes", [])
        user_lower = user_input.lower()

        # Simple keyword matching
        for route in routes:
            patterns = route.get("intent_patterns", [])
            for pattern in patterns:
                if pattern.lower() in user_lower:
                    return route

        return None

    def _inject_playbook_context(self, playbook_path: str) -> str:
        """
        Load playbook YAML and create enriched system prompt.

        Args:
            playbook_path: Path to playbook YAML file (relative to docs/playbook/)

        Returns:
            System prompt with playbook context
        """
        # Prepend docs/playbook/ if not already present
        if not playbook_path.startswith("docs/playbook/"):
            full_path = self.repo_root / "docs" / "playbook" / playbook_path
        else:
            full_path = self.repo_root / playbook_path

        if not full_path.exists():
            logger.warning(f"Playbook file not found: {full_path}")
            return ""

        try:
            with open(full_path) as f:
                playbook = yaml.safe_load(f)
        except Exception as e:
            logger.error(f"Failed to load playbook: {e}")
            return ""

        # Build enriched context
        domain = playbook.get("metadata", {}).get("domain", "general")
        target = playbook.get("target_agent", "VIBE_ALIGNER")
        
        # Format pre-questions
        pre_questions = playbook.get("pre_enrichment", {}).get("pre_questions", [])
        questions_text = ""
        if pre_questions:
            questions_text = "\n**Pre-Questions:**\n"
            for q in pre_questions:
                question = q.get("question", "")
                questions_text += f"  - {question}\n"

        # Format quality gates
        quality_gates = playbook.get("quality_gates", [])
        gates_text = ""
        if quality_gates:
            gates_text = "\n**Quality Gates:**\n"
            for gate in quality_gates[:3]:  # Show top 3
                desc = gate.get("description", "")
                gates_text += f"  - {desc}\n"

        context = f"""
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
üéØ PLAYBOOK CONTEXT LOADED
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

**Playbook:** {playbook_path}
**Domain:** {domain}
**Target Workflow:** {target}
{questions_text}{gates_text}

You are now operating with domain-specific context.
Execute the workflow with domain expertise.

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
"""
        return context

    def _display_boot_prompt(self, registry: Optional[Dict[str, Any]], handoff: Optional[Dict[str, Any]]) -> None:
        """
        Display boot prompt with playbook awareness.

        Args:
            registry: Playbook registry (None if unavailable)
            handoff: Session handoff (None if unavailable)
        """
        print()
        print("‚ïê" * 60)
        print("üöÄ STEWARD READY - PLAYBOOK SYSTEM AVAILABLE")
        print("‚ïê" * 60)
        print()

        if registry:
            routes = registry.get("routes", [])
            # Filter out fallback routes
            main_routes = [r for r in routes if r.get("priority") != "LOW"]
            
            print(f"üìö Available Playbooks: {len(main_routes)} registered")
            print()
            print("Available routes:")
            for route in main_routes[:5]:  # Show top 5
                name = route.get("name", "unknown")
                desc = route.get("description", "")
                print(f"  ‚Ä¢ {name}: {desc}")
            
            if len(main_routes) > 5:
                print(f"  ... and {len(main_routes) - 5} more")
            print()
        
        print("What would you like to do?")
        print("  ‚Üí Describe your intent (e.g., 'restaurant app', 'continue work')")
        print("  ‚Üí Say 'list routes' to see all available playbooks")
        print()
        print("‚ïê" * 60)
        print()

    def boot_mode(self, user_input: Optional[str] = None) -> None:
        """
        Boot mode: System initialization + playbook routing.

        Flow:
        1. Display MOTD (system status) - already shown in main()
        2. Load context from all sources
        3. Route to task based on context/intent
        4. Compose enriched prompt for STEWARD
        5. Output operational prompt

        Note: This is called by ./bin/system-boot.sh to provide
              integrated context for STEWARD initialization.

        Args:
            user_input: Optional user intent for routing
        """
        # Import BootSequence from agency_os runtime
        try:
            sys.path.insert(0, str(self.repo_root / "agency_os" / "00_system" / "runtime"))
            from boot_sequence import BootSequence
            
            # Run the integrated boot sequence
            boot = BootSequence(self.repo_root)
            boot.run(user_input)
            
        except ImportError as e:
            logger.warning(f"‚ö†Ô∏è New boot sequence not available, using legacy mode: {e}")
            # Fallback to legacy boot
            handoff = load_session_handoff()
            registry = self._load_playbook_registry()
            self._display_boot_prompt(registry, handoff)
            logger.info("‚úÖ STEWARD boot sequence complete (legacy mode)")
            logger.info("   Waiting for user intent...")

    def match_intent(self, user_input: str) -> None:
        """
        Match user intent against playbook registry and display result.

        This is a helper command for testing playbook matching.

        Args:
            user_input: User's intent string
        """
        # Load registry
        registry = self._load_playbook_registry()
        
        if not registry:
            print("‚ùå Failed to load playbook registry")
            return

        # Match playbook
        matched = self._match_playbook(user_input, registry)

        if matched:
            print()
            print("‚úÖ PLAYBOOK MATCHED")
            print("‚ïê" * 60)
            print(f"Route: {matched.get('name', 'unknown')}")
            print(f"Description: {matched.get('description', '')}")
            print(f"Priority: {matched.get('priority', 'MEDIUM')}")
            print(f"Target Chain: {matched.get('target_chain', 'unknown')}")
            print()
            
            # Load and display context
            target_chain = matched.get('target_chain', '')
            if target_chain:
                context = self._inject_playbook_context(target_chain)
                if context:
                    print(context)
            
        else:
            print()
            print("‚ö†Ô∏è  NO PLAYBOOK MATCHED")
            print("‚ïê" * 60)
            print(f"Input: '{user_input}'")
            print()
            print("Available routes:")
            routes = registry.get("routes", [])
            for route in routes[:5]:
                name = route.get("name", "unknown")
                patterns = route.get("intent_patterns", [])
                print(f"  ‚Ä¢ {name}: {', '.join(patterns[:3])}")
            print()



def main():
    """CLI entry point"""
    import argparse

    # ========================================================================
    # BOOT SEQUENCE (GAD-005-ADDITION: Multi-Layered Context Injection)
    # ========================================================================
    print("üîê VIBE AGENCY - SYSTEM BOOT")
    print("=" * 50)
    print()

    # LAYER 0: Verify system integrity FIRST (GAD-005-ADDITION)
    print("[Layer 0] Verifying system integrity...")
    integrity_ok = verify_system_integrity()

    if not integrity_ok:
        print()
        print("‚õî BOOT HALTED: System integrity check failed", file=sys.stderr)
        print("   Cannot proceed with compromised regulators.", file=sys.stderr)
        print()
        print("Action Required:", file=sys.stderr)
        print("  1. Run: python scripts/verify-system-integrity.py", file=sys.stderr)
        print("  2. Investigate checksum failures", file=sys.stderr)
        print("  3. Restore files from trusted baseline", file=sys.stderr)
        print("  4. Regenerate manifest if legitimate changes made:", file=sys.stderr)
        print("     python scripts/generate-integrity-manifest.py", file=sys.stderr)
        sys.exit(1)

    print("   ‚úÖ System integrity verified")
    print()

    # LAYER 1: Display MOTD (GAD-005: Unavoidable MOTD)
    print("[Layer 1] Loading session context...")
    try:
        display_motd()
    except Exception as e:
        # Non-fatal - show warning and continue
        print(f"‚ö†Ô∏è  Warning: MOTD display failed: {e}", file=sys.stderr)
        print()

    print()
    print("‚úÖ SYSTEM BOOT COMPLETE")
    print("=" * 50)
    print()

    parser = argparse.ArgumentParser(
        description="VIBE CLI - Agency OS Command Line Interface",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Boot STEWARD with playbook routing (recommended)
  vibe-cli boot

  # Run project in delegated mode (default - MVP)
  vibe-cli run my-project-123

  # Run in autonomous mode (standalone - deferred to v1.1)
  vibe-cli run my-project-123 --mode=autonomous

Notes:
  - MVP uses DELEGATION ONLY mode
  - Claude Code operator provides intelligence (not vibe-cli)
  - No API keys required (Claude Code handles authentication)
        """,
    )

    parser.add_argument("command", type=str, choices=["run", "boot", "match"], help="Command to execute")
    parser.add_argument("project_id", type=str, nargs="?", help="Project ID (required for 'run' command) or user intent (for 'match'/'boot' commands)")
    parser.add_argument(
        "--mode",
        type=str,
        choices=["delegated", "autonomous"],
        default="delegated",
        help="Execution mode (default: delegated)",
    )
    parser.add_argument(
        "--repo-root",
        type=Path,
        default=Path.cwd(),
        help="Repository root (default: current directory)",
    )
    parser.add_argument(
        "--resume",
        action="store_true",
        help="Resume project from last handoff checkpoint",
    )

    args = parser.parse_args()

    # Validate command-specific requirements
    if args.command == "run" and not args.project_id:
        parser.error("'run' command requires project_id argument")
    if args.command == "match" and not args.project_id:
        parser.error("'match' command requires user intent as argument")

    # Initialize CLI
    cli = VibeCLI(repo_root=args.repo_root)

    # Execute command
    if args.command == "run":
        cli.run_project(args.project_id, mode=args.mode, resume=args.resume)
    elif args.command == "boot":
        cli.boot_mode(user_input=args.project_id)  # project_id is reused as optional intent
    elif args.command == "match":
        cli.match_intent(args.project_id)  # project_id is reused as intent string
    else:
        parser.print_help()
        sys.exit(1)


if __name__ == "__main__":
    main()
