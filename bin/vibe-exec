#!/usr/bin/env python3
"""
VIBE Playbook Executor (GAD-9: Playbook Supremacy)
===================================================

The ONE AND ONLY way to execute workflows in vibe-agency.

Architecture:
  1. Load Phoenix Config (GAD-100)
  2. Load Workflow YAML (GAD-903)
  3. Initialize LLM Client (GAD-511)
  4. Create Agents with LLM capability
  5. Setup Agent Router (GAD-904)
  6. Execute via Graph Executor (GAD-902)

Usage:
  ./bin/vibe-exec <workflow.yaml> [context] [--lens <lens_id>]

Examples:
  # Mock execution (default, safe)
  ./bin/vibe-exec agency_os/core_system/playbook/workflows/research_topic.yaml "AI Safety"

  # With semantic lens (GAD-906/907: Intelligence injection)
  ./bin/vibe-exec agency_os/core_system/playbook/workflows/research_topic.yaml "AI Safety" --lens first_principles

  # Live fire execution (requires API key)
  VIBE_LIVE_FIRE=true ./bin/vibe-exec agency_os/core_system/playbook/workflows/research_topic.yaml "AI Safety"

  # Live fire + lens (maximum intelligence)
  VIBE_LIVE_FIRE=true ./bin/vibe-exec agency_os/core_system/playbook/workflows/research_topic.yaml "AI Safety" --lens first_principles

Environment Variables:
  VIBE_LIVE_FIRE=true        Enable real LLM execution (default: false)
  GOOGLE_API_KEY=...         Google Gemini API key
  ANTHROPIC_API_KEY=...      Anthropic Claude API key

Semantic Lenses (GAD-906/907):
  --lens first_principles    Apply First Principles Thinking mindset
  (More lenses coming soon: systems_thinking, user_centric, etc.)

Version: 1.0 (Operation Playbook Supremacy + Mindset Injection)
"""

import logging
import sys
from importlib.util import module_from_spec, spec_from_file_location
from pathlib import Path

# Add repo root to path
repo_root = Path(__file__).parent.parent
sys.path.insert(0, str(repo_root))

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format="%(message)s",
)
logger = logging.getLogger(__name__)


def main():
    """Execute a workflow using the playbook architecture."""

    # Parse arguments (including --lens flag)
    if len(sys.argv) < 2:
        print(__doc__)
        sys.exit(1)

    workflow_path = Path(sys.argv[1])

    # Parse remaining arguments: context and optional --lens flag
    lens_id = None
    context_parts = []

    i = 2
    while i < len(sys.argv):
        if sys.argv[i] == "--lens" and i + 1 < len(sys.argv):
            lens_id = sys.argv[i + 1]
            i += 2
        else:
            context_parts.append(sys.argv[i])
            i += 1

    context = " ".join(context_parts) if context_parts else None

    if not workflow_path.exists():
        print(f"‚ùå Workflow not found: {workflow_path}")
        sys.exit(1)

    print("\n" + "=" * 80)
    print("üéØ VIBE PLAYBOOK EXECUTOR (Operation Playbook Supremacy)")
    print("=" * 80)

    try:
        # STEP 1: Load Phoenix Config (GAD-100)
        print("\nüìç STEP 1: Loading Phoenix Configuration (GAD-100)")
        print("-" * 80)

        from agency_os.config.phoenix import get_config
        config = get_config()

        print(f"  ‚úÖ Phoenix config loaded")
        print(f"     Environment: {config.system.environment}")
        print(f"     Live Fire: {config.safety.live_fire_enabled}")
        print(f"     Provider: {config.model.provider}")
        print(f"     Model: {config.model.model_name}")

        # STEP 2: Load Workflow (GAD-903)
        print("\nüìç STEP 2: Loading Workflow Definition (GAD-903)")
        print("-" * 80)

        # Load workflow loader module from numeric directory
        # Import executor first to set up the module properly
        sys.path.insert(0, str(repo_root / "agency_os" / "core_system" / "playbook"))
        import executor  # noqa: F401 - needed for loader imports
        sys.path.pop(0)  # Remove so we can import normally

        # Now import from the proper package path
        loader_path = repo_root / "agency_os" / "core_system" / "playbook" / "loader.py"
        spec = spec_from_file_location("playbook_loader", loader_path)
        loader_module = module_from_spec(spec)
        sys.modules["playbook_loader"] = loader_module
        spec.loader.exec_module(loader_module)

        WorkflowLoader = loader_module.WorkflowLoader

        loader = WorkflowLoader()
        workflow = loader.load_workflow(workflow_path)

        print(f"  ‚úÖ Workflow loaded: {workflow.name}")
        print(f"     ID: {workflow.id}")
        print(f"     Nodes: {len(workflow.nodes)}")
        print(f"     Entry: {workflow.entry_point}")
        print(f"     Est. Cost: ${workflow.estimated_cost_usd:.2f}")

        # STEP 2.5: Load Semantic Lens (GAD-906/907)
        lens_prompt = None
        if lens_id:
            print("\nüìç STEP 2.5: Loading Semantic Lens (GAD-906/907)")
            print("-" * 80)

            try:
                # Load lens loader module
                lens_loader_path = repo_root / "agency_os" / "core_system" / "knowledge" / "lens_loader.py"
                spec = spec_from_file_location("lens_loader", lens_loader_path)
                lens_module = module_from_spec(spec)
                sys.modules["lens_loader"] = lens_module
                spec.loader.exec_module(lens_module)

                load_lens = lens_module.load_lens
                get_lens_metadata = lens_module.get_lens_metadata

                # Load lens metadata first
                metadata = get_lens_metadata(lens_id)
                if metadata:
                    print(f"  üî¨ Lens: {metadata['name']}")
                    print(f"     ID: {lens_id}")
                    print(f"     Category: {metadata['category']}")
                    print(f"     Description: {metadata['description']}")
                else:
                    print(f"  üî¨ Lens: {lens_id}")

                # Load full lens prompt
                lens_prompt = load_lens(lens_id)
                print(f"  ‚úÖ Lens loaded ({len(lens_prompt)} chars)")
                print(f"     Mindset injection: ACTIVE")

            except Exception as e:
                print(f"  ‚ùå Failed to load lens '{lens_id}': {e}")
                print(f"     Continuing without lens...")
                lens_prompt = None
        else:
            print("\nüìç STEP 2.5: Semantic Lens")
            print("-" * 80)
            print("  ‚ÑπÔ∏è  No lens specified (standard execution)")
            print("     üí° Use --lens first_principles to inject mindset")

        # STEP 3: Initialize LLM Client (GAD-511)
        print("\nüìç STEP 3: Initializing LLM Client (GAD-511)")
        print("-" * 80)

        llm_client = None
        if config.safety.live_fire_enabled:
            try:
                from agency_os.runtime.llm_client import LLMClient

                # Initialize with budget limit from quotas
                llm_client = LLMClient(budget_limit=config.quotas.cost_per_hour_usd)
                provider_name = llm_client.provider.get_provider_name()

                print(f"  ‚úÖ LLM Client initialized: {provider_name}")
                print(f"     Mode: {llm_client.mode}")
                print(f"     Budget: ${config.quotas.cost_per_hour_usd:.2f}")

                if llm_client.mode == "noop":
                    print("     ‚ö†Ô∏è  No API key found - running in mock mode")
                    print("     üí° Set GOOGLE_API_KEY or ANTHROPIC_API_KEY to enable real execution")
            except Exception as e:
                logger.warning(f"  ‚ö†Ô∏è  Could not initialize LLM client: {e}")
                print("  ‚ÑπÔ∏è  Falling back to mock mode")
        else:
            print("  ‚ÑπÔ∏è  Mock mode (safe) - Live fire disabled")
            print("     üí° Set VIBE_LIVE_FIRE=true to enable real execution")

        # STEP 4: Create Agents
        print("\nüìç STEP 4: Creating Agents with LLM Capability")
        print("-" * 80)

        from agency_os.agents.base_agent import ExecutionResult

        class AgentWithLLM:
            """Agent with LLM capability (GAD-511 integration)."""

            def __init__(self, name: str, role: str, capabilities: list, llm_client=None):
                self.name = name
                self.role = role
                self.capabilities = capabilities
                self.llm_client = llm_client

            def can_execute(self, required_skills: list) -> bool:
                """Check if agent can execute required skills."""
                return all(skill in self.capabilities for skill in required_skills)

            def execute_command(self, command: str, prompt: str = None, **kwargs):
                """Execute command, using LLM if available in live fire mode."""
                execution_prompt = prompt or command

                # Try real LLM invocation in live fire mode
                if config.safety.live_fire_enabled and self.llm_client and hasattr(self.llm_client, "mode"):
                    if self.llm_client.mode != "noop":
                        logger.info(f"üî• LIVE FIRE: Invoking real LLM for '{command}'")
                        try:
                            full_prompt = f"{execution_prompt}\n\nProvide a comprehensive, intelligent response."

                            response = self.llm_client.invoke(
                                prompt=full_prompt,
                                max_tokens=4096,
                                temperature=0.7,
                            )

                            logger.info(f"‚úÖ LLM response received ({response.usage.output_tokens} tokens)")

                            result = ExecutionResult(
                                success=True,
                                output=response.content,
                                error="",
                                exit_code=0,
                                duration_ms=0,
                            )
                            from enum import Enum
                            class Status(Enum):
                                SUCCESS = "success"
                            result.status = Status.SUCCESS
                            result.cost_usd = response.usage.cost_usd
                            return result

                        except Exception as e:
                            logger.error(f"‚ùå LLM invocation failed: {e}")
                            # Fall through to mock

                # Mock response (fallback)
                logger.debug(f"Mock mode: {command}")
                result = ExecutionResult(
                    success=True,
                    output=f"[Mock Output] {command}: {execution_prompt}",
                    error="",
                    exit_code=0,
                    duration_ms=0,
                )
                from enum import Enum
                class Status(Enum):
                    SUCCESS = "success"
                result.status = Status.SUCCESS
                result.cost_usd = 0.0
                return result

        # Create agents based on workflow requirements
        agents = []

        # Create a versatile researcher agent
        researcher = AgentWithLLM(
            name="claude-researcher",
            role="Researcher",
            capabilities=[
                "research", "search", "synthesis", "reasoning",
                "documentation", "pattern_recognition",
                "information_analysis", "domain_knowledge",
                "information_retrieval", "web_search",
                "knowledge_synthesis", "writing", "critical_analysis",
                "knowledge_organization"
            ],
            llm_client=llm_client,
        )
        agents.append(researcher)
        print(f"  ‚úÖ {researcher.name} ({researcher.role})")
        print(f"     Capabilities: {len(researcher.capabilities)} skills")

        # Create a coder agent
        coder = AgentWithLLM(
            name="claude-coder",
            role="Code Developer",
            capabilities=[
                "coding", "debugging", "python", "refactoring",
                "testing", "code_analysis", "implement"
            ],
            llm_client=llm_client,
        )
        agents.append(coder)
        print(f"  ‚úÖ {coder.name} ({coder.role})")
        print(f"     Capabilities: {len(coder.capabilities)} skills")

        # STEP 5: Setup Agent Router (GAD-904)
        print("\nüìç STEP 5: Setting Up Agent Router (GAD-904)")
        print("-" * 80)

        # Load router module
        router_path = repo_root / "agency_os" / "core_system" / "playbook" / "router.py"
        spec = spec_from_file_location("playbook_router", router_path)
        router_module = module_from_spec(spec)
        sys.modules["playbook_router"] = router_module
        spec.loader.exec_module(router_module)

        AgentRouter = router_module.AgentRouter

        router = AgentRouter(agents=agents)
        print(f"  ‚úÖ Router initialized with {len(agents)} agent(s)")

        # Show capability matrix
        matrix = router.get_capability_matrix()
        for agent_name, caps in matrix.items():
            print(f"     {agent_name}: {len(caps)} capabilities")

        # STEP 6: Initialize Graph Executor (GAD-902)
        print("\nüìç STEP 6: Creating Workflow Executor (GAD-902)")
        print("-" * 80)

        # Load executor module
        executor_path = repo_root / "agency_os" / "core_system" / "playbook" / "executor.py"
        spec = spec_from_file_location("playbook_executor", executor_path)
        exec_module = module_from_spec(spec)
        sys.modules["playbook_executor"] = exec_module
        spec.loader.exec_module(exec_module)

        GraphExecutor = exec_module.GraphExecutor
        ExecutionStatus = exec_module.ExecutionStatus

        executor = GraphExecutor()
        executor.set_router(router)

        # GAD-906/907: Inject semantic lens if loaded
        if lens_prompt:
            executor.set_lens(lens_prompt)

        print(f"  ‚úÖ Executor initialized")
        print(f"     Workflow: {workflow.id}")
        print(f"     Router: Connected")
        if lens_prompt:
            print(f"     üî¨ Lens: INJECTED (mindset transformation active)")

        # STEP 7: Execute Workflow
        print("\nüìç STEP 7: Executing Workflow")
        print("-" * 80)

        execution_context = context or workflow.intent
        print(f"  Context: {execution_context}")
        print(f"  Mode: {'üî• LIVE FIRE' if config.safety.live_fire_enabled else 'üõ°Ô∏è  MOCK (safe)'}")
        print("")

        execution_log = []
        responses = []

        # Execute workflow nodes in sequence (starting from entry point)
        # For now, execute the entry point node
        # TODO: Full graph traversal with dependency resolution
        for node_id in [workflow.entry_point]:
            node = workflow.nodes[node_id]
            result = executor.execute_step(workflow, node_id, context=execution_context)

            execution_log.append({
                "node_id": node_id,
                "action": node.action,
                "status": result.status.value,
                "cost_usd": result.cost_usd,
            })

            if hasattr(result, "output") and result.output:
                responses.append(result.output)

            status_icon = "‚úÖ" if result.status == ExecutionStatus.SUCCESS else "‚è≥"
            print(f"  {status_icon} [{node_id}] {node.action} ‚Üí {result.status.value}")

        # STEP 8: Execution Summary
        print("\nüìç STEP 8: Execution Summary")
        print("-" * 80)
        print(f"{'Node ID':<30} {'Action':<20} {'Status':<10} {'Cost':<10}")
        print("-" * 80)

        total_cost = 0
        for log in execution_log:
            print(
                f"{log['node_id']:<30} {log['action']:<20} "
                f"{log['status']:<10} ${log['cost_usd']:.4f}"
            )
            total_cost += log["cost_usd"]

        print("-" * 80)
        print(f"{'TOTAL':<30} {'':<20} {'':<10} ${total_cost:.4f}")

        # STEP 9: Display Response
        is_real_response = (
            responses
            and responses[0]
            and isinstance(responses[0], str)
            and "[Mock" not in responses[0]
            and not responses[0].startswith("[")
        )

        if is_real_response:
            print("\nüìç STEP 9: Intelligent Response Generated")
            print("-" * 80)
            for i, response in enumerate(responses, 1):
                print(f"\nüìù Response {i}:")
                print(response)
            print("\n" + "-" * 80)
        else:
            print("\nüìç STEP 9: Workflow Output")
            print("-" * 80)
            if responses:
                for response in responses:
                    print(response)
            else:
                print(f"  Workflow '{workflow.name}' executed successfully")
                print(f"  Mode: {'Live Fire' if config.safety.live_fire_enabled else 'Mock'}")

        # STEP 10: Cost Summary (if live fire)
        if config.safety.live_fire_enabled and llm_client:
            print("\nüìç STEP 10: Cost Summary")
            print("-" * 80)
            cost_summary = llm_client.get_cost_summary()
            print(f"  Total cost: ${cost_summary['total_cost_usd']:.4f}")
            print(
                f"  Tokens used: {cost_summary['total_input_tokens']} input, "
                f"{cost_summary['total_output_tokens']} output"
            )
            print(f"  Invocations: {cost_summary['total_invocations']}")

        # Final status
        all_success = all(log["status"] == "success" for log in execution_log)

        print("\n" + "=" * 80)
        if all_success:
            print("üéâ EXECUTION SUCCESSFUL")
            if config.safety.live_fire_enabled and llm_client and llm_client.mode != "noop":
                print("‚ú® Real intelligence delivered via live LLM execution!")
            else:
                print("‚úÖ Workflow completed (mock mode)")
        else:
            print("‚è≥ EXECUTION IN PROGRESS")
            print("Some steps pending or failed")
        print("=" * 80)

        return 0 if all_success else 1

    except Exception as e:
        print("\n" + "=" * 80)
        print("‚ùå EXECUTION FAILED")
        print(f"Error: {e}")
        print("=" * 80)
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())
